#!/usr/bin/env python3
# preprocess.py
# é¡¹ç›®å®ªæ³•ï¼šWMT14æ•°æ®é¢„å¤„ç†å™¨ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
# ç»å¯¹é€‚é…ç°å®ç¡¬ä»¶ï¼šåˆ†å—å¤„ç† + æµå¼å¤„ç† + è¿›åº¦é€æ˜

import os
import json
import torch
import sentencepiece as spm
from datasets import load_dataset
from tqdm import tqdm
from pathlib import Path
from typing import List, Dict, Tuple, Iterator
import logging
import time

# å¯¼å…¥é…ç½®
import config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)

class WMT14Preprocessor:
    """
    WMT14æ•°æ®é¢„å¤„ç†å™¨ - é¡¹ç›®å®ªæ³•å®ç°
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥ï¼šä½¿ç”¨BPE + å…±äº«è¯æ±‡è¡¨
    2. ç»å¯¹é€‚é…ç°å®ç¡¬ä»¶ï¼šåˆ†å—å¤„ç†ï¼Œé¿å…å†…å­˜çˆ†ç‚¸
    3. ç»å¯¹ä¿¡æ¯é€æ˜ï¼šæ‰€æœ‰æ“ä½œéƒ½æœ‰è¿›åº¦æ¡å’Œæ¸…æ™°æ—¥å¿—
    4. ç»å¯¹å·¥ç¨‹ä¸“ä¸šï¼šé”™è¯¯å¤„ç† + æ–­ç‚¹ç»­ä¼ 
    """
    
    def __init__(self):
        self.prepared_data_dir = Path(config.PREPARED_DATA_DIR)
        self.bpe_model_prefix = config.BPE_MODEL_PREFIX
        self.vocab_size = config.BPE_VOCAB_SIZE
        self.max_seq_len = config.MAX_SEQ_LEN
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.prepared_data_dir.mkdir(exist_ok=True)
        for subdir in ['train_chunks', 'validation_chunks', 'test_chunks']:
            (self.prepared_data_dir / subdir).mkdir(exist_ok=True)
            
        logger.info(f"ğŸš€ åˆå§‹åŒ–WMT14é¢„å¤„ç†å™¨")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.prepared_data_dir}")
        logger.info(f"ğŸ“Š BPEè¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        logger.info(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {self.max_seq_len}")
    
    def train_bpe_model(self) -> spm.SentencePieceProcessor:
        """
        è®­ç»ƒSentencePiece BPEæ¨¡å‹ - ä½¿ç”¨æµå¼å¤„ç†é¿å…å†…å­˜é—®é¢˜
        
        Returns:
            è®­ç»ƒå¥½çš„SentencePieceå¤„ç†å™¨
        """
        bpe_model_path = f"{self.bpe_model_prefix}.model"
        
        if os.path.exists(bpe_model_path):
            logger.info(f"âœ… BPEæ¨¡å‹å·²å­˜åœ¨: {bpe_model_path}")
            sp = spm.SentencePieceProcessor(model_file=bpe_model_path)
            return sp
            
        logger.info("ğŸ”§ å¼€å§‹è®­ç»ƒSentencePiece BPEæ¨¡å‹...")
        logger.info("ğŸ“¡ ä»Hugging Faceæµå¼åŠ è½½WMT14è®­ç»ƒæ•°æ®...")
        
        try:
            # ä½¿ç”¨æµå¼æ•°æ®é›†é¿å…å†…å­˜é—®é¢˜
            dataset_train = load_dataset(
                config.DATASET_NAME, 
                config.LANGUAGE_PAIR, 
                split='train', 
                streaming=True
            )
            
            def get_training_corpus() -> Iterator[str]:
                """ç”Ÿæˆè®­ç»ƒè¯­æ–™çš„è¿­ä»£å™¨ - ä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†"""
                count = 0
                with tqdm(desc="ğŸ”¤ æ”¶é›†BPEè®­ç»ƒè¯­æ–™ (å…¨éƒ¨)", unit="å¥", disable=False) as pbar:
                    for item in dataset_train:
                        translation = item['translation']
                        yield translation[config.SRC_LANGUAGE]  # å¾·è¯­
                        yield translation[config.TGT_LANGUAGE]  # è‹±è¯­
                        count += 2
                        pbar.update(2)
                        
                        # åˆ é™¤æå‰breaké™åˆ¶ï¼Œä½¿ç”¨å…¨éƒ¨è®­ç»ƒé›†è®­ç»ƒBPE
                        # if count >= 2000000:  # å·²åˆ é™¤
                        #     break
                            
                logger.info(f"ğŸ“Š BPEè®­ç»ƒè¯­æ–™æ”¶é›†å®Œæˆ: {count:,} å¥")
            
            # è®­ç»ƒSentencePieceæ¨¡å‹
            logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒBPEæ¨¡å‹...")
            start_time = time.time()
            
            spm.SentencePieceTrainer.train(
                sentence_iterator=get_training_corpus(),
                model_prefix=self.bpe_model_prefix,
                vocab_size=self.vocab_size,
                character_coverage=config.CHARACTER_COVERAGE,
                model_type='bpe',
                pad_id=config.PAD_IDX,
                unk_id=config.UNK_IDX, 
                bos_id=config.BOS_IDX,
                eos_id=config.EOS_IDX,
                hard_vocab_limit=False
            )
            
            training_time = time.time() - start_time
            logger.info(f"âœ… BPEæ¨¡å‹è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.1f}ç§’")
            
            # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
            sp = spm.SentencePieceProcessor(model_file=bpe_model_path)
            
            # éªŒè¯ç‰¹æ®Štoken
            logger.info(f"âœ… BPEæ¨¡å‹éªŒè¯å®Œæˆ - è¯æ±‡è¡¨å¤§å°: {sp.vocab_size():,}")
            # ç®€åŒ–ç‰¹æ®ŠtokenéªŒè¯è¾“å‡º
            token_check = all(sp.piece_to_id(token) == expected_id 
                            for token, expected_id in config.SPECIAL_TOKENS.items())
            if token_check:
                logger.info("ğŸ”¤ ç‰¹æ®ŠtokenéªŒè¯é€šè¿‡")
            else:
                logger.warning("âš ï¸ ç‰¹æ®ŠtokenéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
                
            return sp
            
        except Exception as e:
            logger.error(f"âŒ BPEæ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            raise RuntimeError(f"BPEè®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç£ç›˜ç©ºé—´: {str(e)}")
    
    def process_split_to_chunks(self, split_name: str, sp: spm.SentencePieceProcessor, 
                               chunk_size: int = None) -> Dict[str, any]:
        """
        å¤„ç†æ•°æ®é›†çš„ä¸€ä¸ªåˆ†å‰²ï¼ˆtrain/validation/testï¼‰å¹¶ä¿å­˜ä¸ºåˆ†å—æ–‡ä»¶
        
        Args:
            split_name: æ•°æ®åˆ†å‰²åç§°
            sp: SentencePieceå¤„ç†å™¨
            chunk_size: æ¯ä¸ªåˆ†å—çš„æ ·æœ¬æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨config.CHUNK_SIZEï¼‰
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        if chunk_size is None:
            chunk_size = config.CHUNK_SIZE
        logger.info(f"ğŸ“¦ å¼€å§‹å¤„ç† {split_name} æ•°æ®é›†...")
        
        try:
            # =================== æ ¸å¿ƒä¿®æ”¹ï¼šåŠ è½½æ­£ç¡®çš„åˆ†å‰² ===================
            if split_name == 'validation':
                # æ ‡å‡†éªŒè¯é›†æ˜¯ newstest2013
                # Hugging Face çš„ 'validation' split å°±æ˜¯ newstest2013
                dataset = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='validation')
                logger.info("åŠ è½½æ ‡å‡†éªŒè¯é›†: newstest2013")
            elif split_name == 'test':
                # æ ‡å‡†æµ‹è¯•é›†æ˜¯ newstest2014
                dataset = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='test')
                logger.info("åŠ è½½æ ‡å‡†æµ‹è¯•é›†: newstest2014")
            else:  # split_name == 'train'
                dataset = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='train')
            # =============================================================
            
            total_samples = len(dataset)
            logger.info(f"ğŸ“Š {split_name} æ•°æ®é›†å¤§å°: {total_samples:,} å¥å¯¹")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                'total_samples': 0,
                'filtered_samples': 0,
                'chunks_created': 0,
                'avg_src_len': 0,
                'avg_tgt_len': 0
            }
            
            chunk_dir = self.prepared_data_dir / f"{split_name}_chunks"
            chunk_idx = 0
            current_chunk = []
            
            src_lengths = []
            tgt_lengths = []
            
            # å¤„ç†æ•°æ®
            with tqdm(total=total_samples, desc=f"ğŸ”„ å¤„ç†{split_name}", unit="å¥å¯¹") as pbar:
                for idx, item in enumerate(dataset):
                    translation = item['translation']
                    src_text = translation[config.SRC_LANGUAGE]
                    tgt_text = translation[config.TGT_LANGUAGE]
                    
                    # BPEç¼–ç 
                    src_tokens = sp.encode_as_ids(src_text)
                    tgt_tokens = sp.encode_as_ids(tgt_text)
                    
                    # é•¿åº¦è¿‡æ»¤
                    if (len(src_tokens) > self.max_seq_len or 
                        len(tgt_tokens) > self.max_seq_len or
                        len(src_tokens) < 1 or len(tgt_tokens) < 1):
                        stats['filtered_samples'] += 1
                        pbar.update(1)
                        continue
                    
                    # æ·»åŠ åˆ°å½“å‰åˆ†å—
                    current_chunk.append({
                        'src_tokens': src_tokens,
                        'tgt_tokens': tgt_tokens
                    })
                    
                    src_lengths.append(len(src_tokens))
                    tgt_lengths.append(len(tgt_tokens))
                    stats['total_samples'] += 1
                    
                    # ä¿å­˜åˆ†å—
                    if len(current_chunk) >= chunk_size:
                        self._save_chunk(chunk_dir, chunk_idx, current_chunk)
                        chunk_idx += 1
                        current_chunk = []
                        stats['chunks_created'] += 1
                    
                    pbar.update(1)
            
            # ä¿å­˜æœ€åä¸€ä¸ªåˆ†å—
            if current_chunk:
                self._save_chunk(chunk_dir, chunk_idx, current_chunk)
                stats['chunks_created'] += 1
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if src_lengths:
                stats['avg_src_len'] = sum(src_lengths) / len(src_lengths)
                stats['avg_tgt_len'] = sum(tgt_lengths) / len(tgt_lengths)
            
            logger.info(f"âœ… {split_name} å¤„ç†å®Œæˆ: {stats['total_samples']:,} æ ·æœ¬, {stats['chunks_created']:,} åˆ†å—")
            if stats['filtered_samples'] > 0:
                logger.info(f"ğŸš« è¿‡æ»¤æ ·æœ¬: {stats['filtered_samples']:,}")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†{split_name}æ•°æ®å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
    
    def _save_chunk(self, chunk_dir: Path, chunk_idx: int, chunk_data: List[Dict]):
        """ä¿å­˜ä¸€ä¸ªæ•°æ®åˆ†å—"""
        chunk_file = chunk_dir / f"chunk_{chunk_idx}.pt"
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼
        src_data = [item['src_tokens'] for item in chunk_data]
        tgt_data = [item['tgt_tokens'] for item in chunk_data]
        
        chunk_tensor = {
            'src_data': src_data,
            'tgt_data': tgt_data
        }
        
        torch.save(chunk_tensor, chunk_file)
    
    def save_metadata(self, train_stats: Dict, valid_stats: Dict, test_stats: Dict, 
                     sp: spm.SentencePieceProcessor):
        """ä¿å­˜é¢„å¤„ç†å…ƒæ•°æ®"""
        metadata = {
            'vocab_size': sp.vocab_size(),
            'bpe_model_path': f"{self.bpe_model_prefix}.model",
            'special_tokens': config.SPECIAL_TOKENS,
            'max_seq_len': self.max_seq_len,
            'splits': {
                'train': train_stats,
                'validation': valid_stats,
                'test': test_stats
            },
            'config': {
                'D_MODEL': config.D_MODEL,
                'NHEAD': config.NHEAD,
                'NUM_ENCODER_LAYERS': config.NUM_ENCODER_LAYERS,
                'NUM_DECODER_LAYERS': config.NUM_DECODER_LAYERS
            }
        }
        
        metadata_file = self.prepared_data_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
        logger.info(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    
    def run_full_preprocessing(self):
        """è¿è¡Œå®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹WMT14å®Œæ•´é¢„å¤„ç†æµç¨‹")
        logger.info("="*80)
        
        try:
            # é˜¶æ®µ1: è®­ç»ƒBPEæ¨¡å‹
            logger.info("ğŸ“– é˜¶æ®µ1: è®­ç»ƒBPEæ¨¡å‹")
            sp = self.train_bpe_model()
            
            # é˜¶æ®µ2: å¤„ç†å„ä¸ªæ•°æ®åˆ†å‰²
            logger.info("\nğŸ“¦ é˜¶æ®µ2: å¤„ç†æ•°æ®åˆ†å‰²")
            
            train_stats = self.process_split_to_chunks('train', sp)
            valid_stats = self.process_split_to_chunks('validation', sp)
            test_stats = self.process_split_to_chunks('test', sp)  # å®é™…ä½¿ç”¨validation
            
            # é˜¶æ®µ3: ä¿å­˜å…ƒæ•°æ®
            logger.info("\nğŸ’¾ é˜¶æ®µ3: ä¿å­˜å…ƒæ•°æ®")
            self.save_metadata(train_stats, valid_stats, test_stats, sp)
            
            # æœ€ç»ˆæŠ¥å‘Š
            logger.info("\n" + "="*80)
            logger.info("ğŸ‰ WMT14é¢„å¤„ç†å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®ä¿å­˜ä½ç½®: {self.prepared_data_dir}")
            logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {train_stats['total_samples']:,} å¥å¯¹")
            logger.info(f"ğŸ“Š éªŒè¯æ•°æ®: {valid_stats['total_samples']:,} å¥å¯¹")
            logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®: {test_stats['total_samples']:,} å¥å¯¹")
            logger.info(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {sp.vocab_size():,}")
            logger.info("\nâœ… ç°åœ¨å¯ä»¥è¿è¡Œ 'python train.py' å¼€å§‹è®­ç»ƒ!")
            
        except Exception as e:
            logger.error(f"âŒ é¢„å¤„ç†å¤±è´¥: {str(e)}")
            logger.error("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
            logger.error("  1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            logger.error("  2. ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
            logger.error("  3. ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…")
            raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ WMT14æ•°æ®é¢„å¤„ç†å™¨ - é¡¹ç›®å®ªæ³•å®ç°")
    print("="*80)
    print("ğŸ“‹ æ ¸å¿ƒåŸåˆ™:")
    print("  âœ… ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥: BPE + å…±äº«è¯æ±‡è¡¨")
    print("  âœ… ç»å¯¹é€‚é…ç°å®ç¡¬ä»¶: åˆ†å—å¤„ç† + æµå¼åŠ è½½")
    print("  âœ… ç»å¯¹ä¿¡æ¯é€æ˜: è¿›åº¦æ¡ + è¯¦ç»†æ—¥å¿—")
    print("  âœ… ç»å¯¹å·¥ç¨‹ä¸“ä¸š: é”™è¯¯å¤„ç† + æ–­ç‚¹ç»­ä¼ ")
    print("="*80)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨å¹¶è¿è¡Œ
    preprocessor = WMT14Preprocessor()
    preprocessor.run_full_preprocessing()

if __name__ == "__main__":
    main()