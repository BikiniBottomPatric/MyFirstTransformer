#!/usr/bin/env python3
# preprocess.py
# é¡¹ç›®å®ªæ³•ï¼šWMT14æ•°æ®é¢„å¤„ç†å™¨ - [çœŸæ­£æœ€ç»ˆçš„ã€ç»è¿‡éªŒè¯çš„æ­£ç¡®ç‰ˆæœ¬]

import os
import json
import torch
import sentencepiece as spm
from datasets import load_dataset
from tqdm import tqdm
from pathlib import Path
from typing import List, Dict, Tuple, Iterator
import logging
import time
import config

logging.basicConfig(level=getattr(logging, config.LOG_LEVEL), format=config.LOG_FORMAT)
logger = logging.getLogger(__name__)

class WMT14Preprocessor:
    def __init__(self):
        self.prepared_data_dir = Path(config.PREPARED_DATA_DIR)
        self.bpe_model_prefix = config.BPE_MODEL_PREFIX
        self.vocab_size = config.BPE_VOCAB_SIZE
        self.max_seq_len = config.MAX_SEQ_LEN
        
        self.prepared_data_dir.mkdir(exist_ok=True)
        for subdir in ['train_chunks', 'validation_chunks', 'test_chunks']:
            (self.prepared_data_dir / subdir).mkdir(exist_ok=True)
            
        logger.info(f"ğŸš€ åˆå§‹åŒ–WMT14é¢„å¤„ç†å™¨")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.prepared_data_dir}")
        logger.info(f"ğŸ“Š BPEè¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        logger.info(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {self.max_seq_len}")
    
    def train_bpe_model(self) -> spm.SentencePieceProcessor:
        bpe_model_path = f"{self.bpe_model_prefix}.model"
        if os.path.exists(bpe_model_path):
            logger.info(f"âœ… BPEæ¨¡å‹å·²å­˜åœ¨: {bpe_model_path}")
            return spm.SentencePieceProcessor(model_file=bpe_model_path)
            
        logger.info("ğŸ”§ å¼€å§‹è®­ç»ƒSentencePiece BPEæ¨¡å‹...")
        try:
            dataset_train = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='train', streaming=True)
            
            def get_training_corpus() -> Iterator[str]:
                with tqdm(desc="ğŸ”¤ æ”¶é›†BPEè®­ç»ƒè¯­æ–™ (å…¨éƒ¨)", unit="å¥") as pbar:
                    for item in dataset_train:
                        translation = item['translation']
                        yield translation[config.SRC_LANGUAGE]
                        yield translation[config.TGT_LANGUAGE]
                        pbar.update(2)

            logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒBPEæ¨¡å‹...")
            start_time = time.time()
            spm.SentencePieceTrainer.train(
                sentence_iterator=get_training_corpus(),
                model_prefix=self.bpe_model_prefix,
                vocab_size=self.vocab_size,
                character_coverage=config.CHARACTER_COVERAGE,
                model_type='bpe',
                pad_id=config.PAD_IDX, unk_id=config.UNK_IDX, 
                bos_id=config.BOS_IDX, eos_id=config.EOS_IDX,
                hard_vocab_limit=False
            )
            logger.info(f"âœ… BPEæ¨¡å‹è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time() - start_time:.1f}ç§’")
            
            sp = spm.SentencePieceProcessor(model_file=bpe_model_path)
            logger.info(f"âœ… BPEæ¨¡å‹éªŒè¯å®Œæˆ - è¯æ±‡è¡¨å¤§å°: {sp.vocab_size():,}")
            return sp
        except Exception as e:
            logger.error(f"âŒ BPEæ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            raise

    def process_split_to_chunks(self, split_name: str, sp: spm.SentencePieceProcessor, chunk_size: int = None) -> Dict[str, any]:
        if chunk_size is None:
            chunk_size = config.CHUNK_SIZE
        logger.info(f"ğŸ“¦ å¼€å§‹å¤„ç† {split_name} æ•°æ®é›†...")
        
        try:
            # =================== å…³é”®ä¿®æ­£ ===================
            # è¿™ä¸ª if/elif/else ç»“æ„ç¡®ä¿äº†æ¯ä¸ªsplit_nameéƒ½åŠ è½½æ­£ç¡®çš„æ•°æ®æº
            if split_name == 'validation':
                dataset = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='validation')
                logger.info("åŠ è½½æ ‡å‡†éªŒè¯é›†: newstest2013")
            elif split_name == 'test':
                dataset = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='test')
                logger.info("åŠ è½½æ ‡å‡†æµ‹è¯•é›†: newstest2014")
            else: # 'train'
                dataset = load_dataset(config.DATASET_NAME, config.LANGUAGE_PAIR, split='train')
            # ===============================================
            
            total_samples = len(dataset)
            stats = {'total_samples': 0, 'filtered_samples': 0, 'chunks_created': 0, 'avg_src_len': 0, 'avg_tgt_len': 0}
            chunk_dir = self.prepared_data_dir / f"{split_name}_chunks"
            chunk_idx, current_chunk, src_lengths, tgt_lengths = 0, [], [], []
            
            with tqdm(total=total_samples, desc=f"ğŸ”„ å¤„ç†{split_name}", unit="å¥å¯¹") as pbar:
                for item in dataset:
                    pbar.update(1)
                    translation = item['translation']
                    src_tokens = sp.encode_as_ids(translation[config.SRC_LANGUAGE])
                    tgt_tokens = sp.encode_as_ids(translation[config.TGT_LANGUAGE])
                    
                    if not (1 <= len(src_tokens) <= self.max_seq_len and 1 <= len(tgt_tokens) <= self.max_seq_len):
                        stats['filtered_samples'] += 1
                        continue
                    
                    current_chunk.append({'src_tokens': src_tokens, 'tgt_tokens': tgt_tokens})
                    src_lengths.append(len(src_tokens))
                    tgt_lengths.append(len(tgt_tokens))
                    stats['total_samples'] += 1
                    
                    if len(current_chunk) >= chunk_size:
                        self._save_chunk(chunk_dir, chunk_idx, current_chunk)
                        chunk_idx += 1
                        current_chunk = []
                        stats['chunks_created'] += 1
            
            if current_chunk:
                self._save_chunk(chunk_dir, chunk_idx, current_chunk)
                stats['chunks_created'] += 1
            
            if src_lengths:
                stats['avg_src_len'] = sum(src_lengths) / len(src_lengths)
                stats['avg_tgt_len'] = sum(tgt_lengths) / len(tgt_lengths)
            
            logger.info(f"âœ… {split_name} å¤„ç†å®Œæˆ: {stats['total_samples']:,} æ ·æœ¬, {stats['chunks_created']:,} åˆ†å—")
            if stats['filtered_samples'] > 0:
                logger.info(f"ğŸš« è¿‡æ»¤æ ·æœ¬: {stats['filtered_samples']:,}")
            return stats
        except Exception as e:
            logger.error(f"âŒ å¤„ç†{split_name}æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def _save_chunk(self, chunk_dir: Path, chunk_idx: int, chunk_data: List[Dict]):
        chunk_file = chunk_dir / f"chunk_{chunk_idx}.pt"
        torch.save({
            'src_data': [item['src_tokens'] for item in chunk_data],
            'tgt_data': [item['tgt_tokens'] for item in chunk_data]
        }, chunk_file)
    
    def save_metadata(self, train_stats: Dict, valid_stats: Dict, test_stats: Dict, sp: spm.SentencePieceProcessor):
        metadata = {
            'vocab_size': sp.vocab_size(),
            'bpe_model_path': f"{self.bpe_model_prefix}.model",
            'special_tokens': config.SPECIAL_TOKENS,
            'max_seq_len': self.max_seq_len,
            'splits': {'train': train_stats, 'validation': valid_stats, 'test': test_stats},
            'config': {'D_MODEL': config.D_MODEL, 'NHEAD': config.NHEAD,
                       'NUM_ENCODER_LAYERS': config.NUM_ENCODER_LAYERS,
                       'NUM_DECODER_LAYERS': config.NUM_DECODER_LAYERS}
        }
        with open(self.prepared_data_dir / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {self.prepared_data_dir / 'metadata.json'}")
    
    def run_full_preprocessing(self):
        logger.info("ğŸš€ å¼€å§‹WMT14å®Œæ•´é¢„å¤„ç†æµç¨‹")
        logger.info("="*80)
        try:
            sp = self.train_bpe_model()
            
            train_stats = self.process_split_to_chunks('train', sp)
            valid_stats = self.process_split_to_chunks('validation', sp)
            test_stats = self.process_split_to_chunks('test', sp)
            
            self.save_metadata(train_stats, valid_stats, test_stats, sp)
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ‰ WMT14é¢„å¤„ç†å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®ä¿å­˜ä½ç½®: {self.prepared_data_dir}")
            logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {train_stats['total_samples']:,} å¥å¯¹")
            logger.info(f"ğŸ“Š éªŒè¯æ•°æ®: {valid_stats['total_samples']:,} å¥å¯¹")
            logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®: {test_stats['total_samples']:,} å¥å¯¹")
            logger.info(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {sp.vocab_size():,}")
            logger.info("\nâœ… ç°åœ¨å¯ä»¥è¿è¡Œ 'python train.py' å¼€å§‹è®­ç»ƒ!")
            
        except Exception as e:
            logger.error(f"âŒ é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise

def main():
    print("ğŸš€ WMT14æ•°æ®é¢„å¤„ç†å™¨ - é¡¹ç›®å®ªæ³•å®ç°")
    print("="*80)
    preprocessor = WMT14Preprocessor()
    preprocessor.run_full_preprocessing()

if __name__ == "__main__":
    main()