#!/usr/bin/env python3
# model.py
# é¡¹ç›®å®ªæ³•ï¼šTransformeræ¨¡å‹ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
# ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥ï¼š100%æ‰‹å†™æ ¸å¿ƒModule + è®ºæ–‡æ ‡å‡†æ¶æ„

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple
import logging

# å¯¼å…¥é…ç½®
import config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)

class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç  - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
    
    ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚
    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    
    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # è®¡ç®—é™¤æ•°é¡¹ï¼š10000^(2i/d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        # åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [seq_len, batch_size, d_model] æˆ– [batch_size, seq_len, d_model]
        Returns:
            æ·»åŠ ä½ç½®ç¼–ç åçš„å¼ é‡
        """
        if x.dim() == 3 and x.size(0) != x.size(1):  # åˆ¤æ–­æ˜¯å¦ä¸º [batch_size, seq_len, d_model]
            # è½¬æ¢ä¸º [seq_len, batch_size, d_model]
            x = x.transpose(0, 1)
            seq_len = x.size(0)
            # ç¡®ä¿ä½ç½®ç¼–ç é•¿åº¦è¶³å¤Ÿ
            if seq_len > self.pe.size(0):
                # å¦‚æœåºåˆ—é•¿åº¦è¶…è¿‡é¢„è®¾æœ€å¤§é•¿åº¦ï¼Œåªä½¿ç”¨å¯ç”¨çš„ä½ç½®ç¼–ç 
                x[:self.pe.size(0)] = x[:self.pe.size(0)] + self.pe
            else:
                x = x + self.pe[:seq_len, :]
            x = x.transpose(0, 1)  # è½¬æ¢å› [batch_size, seq_len, d_model]
        else:
            # å‡è®¾è¾“å…¥ä¸º [seq_len, batch_size, d_model]
            seq_len = x.size(0)
            if seq_len > self.pe.size(0):
                # å¦‚æœåºåˆ—é•¿åº¦è¶…è¿‡é¢„è®¾æœ€å¤§é•¿åº¦ï¼Œåªä½¿ç”¨å¯ç”¨çš„ä½ç½®ç¼–ç 
                x[:self.pe.size(0)] = x[:self.pe.size(0)] + self.pe
            else:
                x = x + self.pe[:seq_len, :]
        
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
    
    Attention(Q,K,V) = softmax(QK^T / sqrt(d_k))V
    MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O
    where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
    """
    
    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % nhead == 0, f"d_model ({d_model}) å¿…é¡»èƒ½è¢« nhead ({nhead}) æ•´é™¤"
        
        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead  # æ¯ä¸ªå¤´çš„ç»´åº¦
        
        # çº¿æ€§æŠ•å½±å±‚
        self.w_q = nn.Linear(d_model, d_model, bias=False)
        self.w_k = nn.Linear(d_model, d_model, bias=False)
        self.w_v = nn.Linear(d_model, d_model, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        
        self.dropout = nn.Dropout(dropout)
        
        # ç¼©æ”¾å› å­
        self.scale = math.sqrt(self.d_k)
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            query: [batch_size, seq_len_q, d_model]
            key: [batch_size, seq_len_k, d_model]
            value: [batch_size, seq_len_v, d_model]
            mask: [batch_size, seq_len_q, seq_len_k] æˆ– [seq_len_q, seq_len_k] æˆ– [batch_size, seq_len]
        
        Returns:
            output: [batch_size, seq_len_q, d_model]
            attention_weights: [batch_size, nhead, seq_len_q, seq_len_k]
        """
        batch_size, seq_len_q, _ = query.size()
        seq_len_k = key.size(1)
        seq_len_v = value.size(1)
        
        # çº¿æ€§æŠ•å½±å¹¶é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = self.w_q(query).view(batch_size, seq_len_q, self.nhead, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len_k, self.nhead, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len_v, self.nhead, self.d_k).transpose(1, 2)
        # å½¢çŠ¶: [batch_size, nhead, seq_len, d_k]
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        # å½¢çŠ¶: [batch_size, nhead, seq_len_q, seq_len_k]
        
        # åº”ç”¨æ©ç 
        if mask is not None:
            # å¤„ç†ä¸åŒç»´åº¦çš„æ©ç 
            if mask.dim() == 2:  # [seq_len_q, seq_len_k] æˆ– [batch_size, seq_len]
                if mask.size(0) == seq_len_q and mask.size(1) == seq_len_k:
                    # å› æœæ©ç : [seq_len_q, seq_len_k] -> [1, 1, seq_len_q, seq_len_k]
                    mask = mask.unsqueeze(0).unsqueeze(0)
                elif mask.size(0) == batch_size:
                    # å¡«å……æ©ç : [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]
                    mask = mask.unsqueeze(1).unsqueeze(2)
                    # æ‰©å±•åˆ°æ‰€æœ‰æŸ¥è¯¢ä½ç½®: [batch_size, 1, seq_len_q, seq_len]
                    mask = mask.expand(batch_size, 1, seq_len_q, seq_len_k)
            elif mask.dim() == 3:  # [batch_size, seq_len_q, seq_len_k]
                mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len_q, seq_len_k]
            
            # å°†æ©ç ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°å€¼
        context = torch.matmul(attention_weights, V)
        # å½¢çŠ¶: [batch_size, nhead, seq_len_q, d_k]
        
        # é‡å¡‘å¹¶è¿æ¥å¤šå¤´
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len_q, self.d_model
        )
        
        # æœ€ç»ˆçº¿æ€§æŠ•å½±
        output = self.w_o(context)
        
        return output, attention_weights

class PositionwiseFeedforward(nn.Module):
    """
    ä½ç½®å‰é¦ˆç½‘ç»œ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
    
    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
    
    ä¸¤å±‚çº¿æ€§å˜æ¢ï¼Œä¸­é—´ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€‚
    å†…å±‚ç»´åº¦é€šå¸¸æ˜¯æ¨¡å‹ç»´åº¦çš„4å€ã€‚
    """
    
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, seq_len, d_model]
        Returns:
            [batch_size, seq_len, d_model]
        """
        return self.linear2(self.dropout(F.relu(self.linear1(x))))

class EncoderLayer(nn.Module):
    """
    Transformerç¼–ç å™¨å±‚ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
    
    æ¯å±‚åŒ…å«ï¼š
    1. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
    2. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
    3. ä½ç½®å‰é¦ˆç½‘ç»œ
    4. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
    """
    
    def __init__(self, d_model: int, nhead: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)
        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            src: [batch_size, seq_len, d_model]
            src_mask: [batch_size, seq_len, seq_len] æˆ– [seq_len, seq_len]
        Returns:
            [batch_size, seq_len, d_model]
        """
        # å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_output, _ = self.self_attn(src, src, src, src_mask)
        src = self.norm1(src + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(src)
        src = self.norm2(src + self.dropout(ff_output))
        
        return src

class DecoderLayer(nn.Module):
    """
    Transformerè§£ç å™¨å±‚ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
    
    æ¯å±‚åŒ…å«ï¼š
    1. æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
    2. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
    3. ç¼–ç å™¨-è§£ç å™¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    4. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
    5. ä½ç½®å‰é¦ˆç½‘ç»œ
    6. æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
    """
    
    def __init__(self, d_model: int, nhead: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)
        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)
        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, tgt: torch.Tensor, memory: torch.Tensor,
                tgt_mask: Optional[torch.Tensor] = None,
                memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            tgt: [batch_size, tgt_seq_len, d_model]
            memory: [batch_size, src_seq_len, d_model] (ç¼–ç å™¨è¾“å‡º)
            tgt_mask: [tgt_seq_len, tgt_seq_len] (å› æœæ©ç )
            memory_mask: [batch_size, tgt_seq_len, src_seq_len] (æºåºåˆ—æ©ç )
        Returns:
            [batch_size, tgt_seq_len, d_model]
        """
        # æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        self_attn_output, _ = self.self_attn(tgt, tgt, tgt, tgt_mask)
        tgt = self.norm1(tgt + self.dropout(self_attn_output))
        
        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        cross_attn_output, _ = self.cross_attn(tgt, memory, memory, memory_mask)
        tgt = self.norm2(tgt + self.dropout(cross_attn_output))
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        ff_output = self.feed_forward(tgt)
        tgt = self.norm3(tgt + self.dropout(ff_output))
        
        return tgt

class TransformerEncoder(nn.Module):
    """
    Transformerç¼–ç å™¨ - å¤šå±‚ç¼–ç å™¨å±‚çš„å †å 
    """
    
    def __init__(self, encoder_layer: EncoderLayer, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])
        self.num_layers = num_layers
    
    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            src: [batch_size, seq_len, d_model]
            src_mask: [batch_size, seq_len, seq_len] æˆ– [seq_len, seq_len]
        Returns:
            [batch_size, seq_len, d_model]
        """
        output = src
        for layer in self.layers:
            output = layer(output, src_mask)
        return output

class TransformerDecoder(nn.Module):
    """
    Transformerè§£ç å™¨ - å¤šå±‚è§£ç å™¨å±‚çš„å †å 
    """
    
    def __init__(self, decoder_layer: DecoderLayer, num_layers: int):
        super().__init__()
        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])
        self.num_layers = num_layers
    
    def forward(self, tgt: torch.Tensor, memory: torch.Tensor,
                tgt_mask: Optional[torch.Tensor] = None,
                memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            tgt: [batch_size, tgt_seq_len, d_model]
            memory: [batch_size, src_seq_len, d_model]
            tgt_mask: [tgt_seq_len, tgt_seq_len]
            memory_mask: [batch_size, tgt_seq_len, src_seq_len]
        Returns:
            [batch_size, tgt_seq_len, d_model]
        """
        output = tgt
        for layer in self.layers:
            output = layer(output, memory, tgt_mask, memory_mask)
        return output

class Transformer(nn.Module):
    """
    å®Œæ•´çš„Transformeræ¨¡å‹ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
    
    åŒ…å«ï¼š
    1. è¯åµŒå…¥å±‚ï¼ˆæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å…±äº«æˆ–åˆ†ç¦»ï¼‰
    2. ä½ç½®ç¼–ç 
    3. Transformerç¼–ç å™¨
    4. Transformerè§£ç å™¨
    5. è¾“å‡ºæŠ•å½±å±‚
    """
    
    def __init__(self, vocab_size: int, d_model: int = 512, nhead: int = 8,
                 num_encoder_layers: int = 6, num_decoder_layers: int = 6,
                 d_ff: int = 2048, max_seq_len: int = 5000, dropout: float = 0.1,
                 share_embeddings: bool = True):
        super().__init__()
        
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.share_embeddings = share_embeddings
        
        # è¯åµŒå…¥å±‚
        if share_embeddings:
            # æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å…±äº«è¯åµŒå…¥ï¼ˆè®ºæ–‡æ¨èï¼‰
            self.embedding = nn.Embedding(vocab_size, d_model)
            self.src_embedding = self.embedding
            self.tgt_embedding = self.embedding
        else:
            # åˆ†ç¦»çš„è¯åµŒå…¥
            self.src_embedding = nn.Embedding(vocab_size, d_model)
            self.tgt_embedding = nn.Embedding(vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)
        
        # ç¼–ç å™¨å’Œè§£ç å™¨
        encoder_layer = EncoderLayer(d_model, nhead, d_ff, dropout)
        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)
        
        decoder_layer = DecoderLayer(d_model, nhead, d_ff, dropout)
        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # å…ˆåˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        # ç„¶åè¿›è¡Œæƒé‡ç»‘å®šï¼ˆå¿…é¡»åœ¨åˆå§‹åŒ–ä¹‹åï¼‰
        if share_embeddings:
            self.output_projection.weight = self.embedding.weight
        
        logger.info(f"ğŸš€ Transformeræ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°: vocab_size={vocab_size}, d_model={d_model}, nhead={nhead}")
        logger.info(f"ğŸ“Š å±‚æ•°: encoder={num_encoder_layers}, decoder={num_decoder_layers}")
        logger.info(f"ğŸ“Š å‚æ•°æ€»æ•°: {self.count_parameters():,}")
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ– - éµå¾ªè®ºæ–‡å»ºè®®"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def count_parameters(self) -> int:
        """è®¡ç®—æ¨¡å‹å‚æ•°æ€»æ•°"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def create_padding_mask(self, seq: torch.Tensor, pad_idx: int) -> torch.Tensor:
        """åˆ›å»ºå¡«å……æ©ç """
        return (seq != pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
    
    def create_causal_mask(self, size: int) -> torch.Tensor:
        """åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰"""
        mask = torch.tril(torch.ones(size, size, dtype=torch.bool))
        return mask  # [size, size]
    
    def forward(self, src: torch.Tensor, tgt: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None,
                memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            src: [batch_size, src_seq_len] æºåºåˆ—token IDs
            tgt: [batch_size, tgt_seq_len] ç›®æ ‡åºåˆ—token IDs
            src_mask: [batch_size, src_seq_len] æˆ– [tgt_seq_len, tgt_seq_len] æºåºåˆ—å¡«å……æ©ç æˆ–å› æœæ©ç 
            tgt_mask: [tgt_seq_len, tgt_seq_len] ç›®æ ‡åºåˆ—å› æœæ©ç 
            memory_mask: [batch_size, tgt_seq_len, src_seq_len] äº¤å‰æ³¨æ„åŠ›æ©ç 
        
        Returns:
            [batch_size, tgt_seq_len, vocab_size] è¾“å‡ºlogits
        """
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        
        src_emb = self.pos_encoding(src_emb)
        tgt_emb = self.pos_encoding(tgt_emb)
        
        # ç¼–ç å™¨
        memory = self.encoder(src_emb, src_mask)
        
        # è§£ç å™¨
        output = self.decoder(tgt_emb, memory, tgt_mask, memory_mask)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.output_projection(output)
        
        return logits
    
    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ä»…ç¼–ç å™¨å‰å‘ä¼ æ’­ï¼ˆç”¨äºæ¨ç†ï¼‰"""
        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)
        src_emb = self.pos_encoding(src_emb)
        return self.encoder(src_emb, src_mask)
    
    def decode(self, tgt: torch.Tensor, memory: torch.Tensor,
               tgt_mask: Optional[torch.Tensor] = None,
               memory_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """ä»…è§£ç å™¨å‰å‘ä¼ æ’­ï¼ˆç”¨äºæ¨ç†ï¼‰"""
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt_emb = self.pos_encoding(tgt_emb)
        output = self.decoder(tgt_emb, memory, tgt_mask, memory_mask)
        return self.output_projection(output)

def create_transformer_model(vocab_size: int) -> Transformer:
    """
    åˆ›å»ºTransformeræ¨¡å‹ - ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°
    
    Args:
        vocab_size: è¯æ±‡è¡¨å¤§å°
    
    Returns:
        Transformeræ¨¡å‹å®ä¾‹
    """
    # ç¡®ä¿ä½ç½®ç¼–ç é•¿åº¦è¶³å¤Ÿæ”¯æŒbeam searchçš„æœ€å¤§è§£ç é•¿åº¦
    max_pos_len = max(config.MAX_SEQ_LEN, config.MAX_DECODE_LENGTH + 50)  # é¢å¤–50ä¸ªä½ç½®ä½œä¸ºç¼“å†²
    
    model = Transformer(
        vocab_size=vocab_size,
        d_model=config.D_MODEL,
        nhead=config.NHEAD,
        num_encoder_layers=config.NUM_ENCODER_LAYERS,
        num_decoder_layers=config.NUM_DECODER_LAYERS,
        d_ff=config.D_FF,
        max_seq_len=max_pos_len,
        dropout=config.DROPOUT,
        share_embeddings=config.SHARE_EMBEDDINGS
    )
    
    logger.info(f"âœ… Transformeræ¨¡å‹åˆ›å»ºæˆåŠŸ")
    logger.info(f"ğŸ“Š æ¨¡å‹é…ç½®: Base Model (d_model={config.D_MODEL}, nhead={config.NHEAD})")
    logger.info(f"ğŸ“Š å‚æ•°ä¼°è®¡: {model.count_parameters() / 1e6:.1f}M")
    
    return model

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•æ¨¡å‹"""
    print("ğŸš€ Transformeræ¨¡å‹æµ‹è¯• - é¡¹ç›®å®ªæ³•å®ç°")
    print("="*80)
    print("ğŸ“‹ æ ¸å¿ƒåŸåˆ™:")
    print("  âœ… ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥: 100%æ‰‹å†™æ ¸å¿ƒModule")
    print("  âœ… ä¸¥æ ¼è®ºæ–‡æ¶æ„: MultiHeadAttention + EncoderLayer + DecoderLayer")
    print("  âœ… æƒé‡åˆå§‹åŒ–: Xavier Uniform")
    print("  âœ… ä½ç½®ç¼–ç : æ­£å¼¦ä½™å¼¦å‡½æ•°")
    print("="*80)
    
    try:
        # åˆ›å»ºæµ‹è¯•æ¨¡å‹
        vocab_size = 37000  # BPEè¯æ±‡è¡¨å¤§å°
        model = create_transformer_model(vocab_size)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size = 2
        src_seq_len = 10
        tgt_seq_len = 8
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        src = torch.randint(0, vocab_size, (batch_size, src_seq_len))
        tgt = torch.randint(0, vocab_size, (batch_size, tgt_seq_len))
        
        # åˆ›å»ºæ©ç 
        src_mask = model.create_padding_mask(src, config.PAD_IDX)
        tgt_mask = model.create_causal_mask(tgt_seq_len)
        
        print(f"\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
        print(f"ğŸ“Š è¾“å…¥å½¢çŠ¶:")
        print(f"  src: {src.shape}")
        print(f"  tgt: {tgt.shape}")
        print(f"  src_mask: {src_mask.shape}")
        print(f"  tgt_mask: {tgt_mask.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(src, tgt, src_mask, tgt_mask)
        
        print(f"ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"ğŸ“Š è¾“å‡ºèŒƒå›´: [{output.min().item():.3f}, {output.max().item():.3f}]")
        
        # æµ‹è¯•ç¼–ç å™¨å’Œè§£ç å™¨åˆ†ç¦»
        print(f"\nğŸ§ª æµ‹è¯•ç¼–ç å™¨/è§£ç å™¨åˆ†ç¦»...")
        with torch.no_grad():
            memory = model.encode(src, src_mask)
            decoder_output = model.decode(tgt, memory, tgt_mask)
        
        print(f"ğŸ“Š ç¼–ç å™¨è¾“å‡º: {memory.shape}")
        print(f"ğŸ“Š è§£ç å™¨è¾“å‡º: {decoder_output.shape}")
        
        # éªŒè¯è¾“å‡ºä¸€è‡´æ€§
        diff = torch.abs(output - decoder_output).max().item()
        print(f"ğŸ“Š è¾“å‡ºä¸€è‡´æ€§æ£€æŸ¥: æœ€å¤§å·®å¼‚ = {diff:.6f}")
        
        if diff < 1e-5:
            print("âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡!")
        else:
            print("âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: è¾“å‡ºä¸ä¸€è‡´")
        
        print(f"\nğŸ¯ æ¨¡å‹å·²å‡†å¤‡å°±ç»ªï¼Œå¯ç”¨äºè®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()