#!/usr/bin/env python3
# data_utils.py (æœ€ç»ˆä¼˜åŒ–ç‰ˆ)
# é¡¹ç›®å®ªæ³•ï¼šWMT14æ•°æ®åŠ è½½å™¨ - ä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡
# ç»å¯¹é€‚é…ç°å®ç¡¬ä»¶ï¼šåˆ†å—åŠ è½½ + å†…å­˜é«˜æ•ˆ + æ‰¹å¤„ç†ä¼˜åŒ–

import os
import json
import torch
import random
import math
import numpy as np
import sentencepiece as spm
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from torch.utils.data import Dataset, DataLoader, Sampler
from torch.nn.utils.rnn import pad_sequence
import logging
from tqdm import tqdm

# å¯¼å…¥é…ç½®
import config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)

class WMT14ChunkedDataset(Dataset):
    """
    WMT14åˆ†å—æ•°æ®é›† - é¡¹ç›®å®ªæ³•å®ç°
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥ï¼šBOS/EOSå¤„ç† + æ­£ç¡®çš„åºåˆ—æ ¼å¼
    2. ç»å¯¹é€‚é…ç°å®ç¡¬ä»¶ï¼šåˆ†å—åŠ è½½ï¼Œé¿å…å†…å­˜çˆ†ç‚¸
    3. ç»å¯¹ä¿¡æ¯é€æ˜ï¼šåŠ è½½è¿›åº¦ + æ¸…æ™°é”™è¯¯ä¿¡æ¯
    4. ç»å¯¹å·¥ç¨‹ä¸“ä¸šï¼šç¼“å­˜æœºåˆ¶ + é”™è¯¯å¤„ç†
    """
    
    def __init__(self, split_name: str, shuffle_chunks: bool = True):
        """
        åˆå§‹åŒ–åˆ†å—æ•°æ®é›†
        
        Args:
            split_name: æ•°æ®åˆ†å‰²åç§° ('train', 'validation', 'test')
            shuffle_chunks: æ˜¯å¦æ‰“ä¹±åˆ†å—é¡ºåº
        """
        self.split_name = split_name
        self.shuffle_chunks = shuffle_chunks
        self.prepared_data_dir = Path(config.PREPARED_DATA_DIR)
        
        # åŠ è½½å…ƒæ•°æ®
        self._load_metadata()
        
        # åŠ è½½BPEæ¨¡å‹
        self._load_bpe_model()
        
        # å‘ç°åˆ†å—æ–‡ä»¶
        self._discover_chunks()
        
        # å½“å‰åŠ è½½çš„åˆ†å—
        self.current_chunk_idx = -1
        self.current_chunk_data = []
        self.current_chunk_size = 0
        
        # å…¨å±€æ ·æœ¬ç´¢å¼•æ˜ å°„
        self._build_sample_index()
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–{split_name}æ•°æ®é›†")
        logger.info(f"ğŸ“¦ åˆ†å—æ•°é‡: {len(self.chunk_files)}")
        logger.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {self.total_samples:,}")
        logger.info(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
    
    def _load_metadata(self):
        """åŠ è½½é¢„å¤„ç†å…ƒæ•°æ®"""
        metadata_file = self.prepared_data_dir / "metadata.json"
        
        if not metadata_file.exists():
            raise FileNotFoundError(
                f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}\n"
                f"ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python preprocess.py' è¿›è¡Œæ•°æ®é¢„å¤„ç†"
            )
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            
            self.vocab_size = self.metadata['vocab_size']
            self.bpe_model_path = self.metadata['bpe_model_path']
            self.special_tokens = self.metadata['special_tokens']
            self.max_seq_len = self.metadata['max_seq_len']
            
            logger.info(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸ: {metadata_file}")
            
        except Exception as e:
            raise RuntimeError(f"âŒ å…ƒæ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
    
    def _load_bpe_model(self):
        """åŠ è½½BPEæ¨¡å‹"""
        if not os.path.exists(self.bpe_model_path):
            raise FileNotFoundError(
                f"âŒ BPEæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.bpe_model_path}\n"
                f"ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python preprocess.py' è¿›è¡Œæ•°æ®é¢„å¤„ç†"
            )
        
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºHugging Face tokenizeræ ¼å¼
            if self.bpe_model_path.endswith('.json'):
                from transformers import AutoTokenizer
                tokenizer_dir = os.path.dirname(self.bpe_model_path)
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
                
                # éªŒè¯ç‰¹æ®Štokenï¼ˆå¤„ç†tokenåç§°æ˜ å°„ï¼‰
                vocab = self.tokenizer.get_vocab()
                token_mapping = {
                    '<s>': '<bos>',  # æ˜ å°„<s>åˆ°<bos>
                    '</s>': '<eos>'  # æ˜ å°„</s>åˆ°<eos>
                }
                
                for token, expected_id in self.special_tokens.items():
                    # å°è¯•åŸå§‹tokenåç§°
                    actual_id = vocab.get(token, -1)
                    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•æ˜ å°„çš„tokenåç§°
                    if actual_id == -1 and token in token_mapping:
                        actual_id = vocab.get(token_mapping[token], -1)
                    
                    if actual_id != expected_id:
                        logger.warning(f"âš ï¸ ç‰¹æ®Štokenä¸åŒ¹é…: {token} æœŸæœ›={expected_id}, å®é™…={actual_id}")
                    else:
                        logger.info(f"âœ… ç‰¹æ®Štokenæ˜ å°„æ­£ç¡®: {token} -> {actual_id}")
                
                logger.info(f"âœ… Hugging Face tokenizeråŠ è½½æˆåŠŸ: {self.bpe_model_path}")
            else:
                # SentencePieceæ ¼å¼
                import sentencepiece as spm
                self.sp = spm.SentencePieceProcessor(model_file=self.bpe_model_path)
                
                # éªŒè¯ç‰¹æ®Štoken
                for token, expected_id in self.special_tokens.items():
                    actual_id = self.sp.piece_to_id(token)
                    if actual_id != expected_id:
                        logger.warning(f"âš ï¸ ç‰¹æ®Štokenä¸åŒ¹é…: {token} æœŸæœ›={expected_id}, å®é™…={actual_id}")
                
                logger.info(f"âœ… SentencePieceæ¨¡å‹åŠ è½½æˆåŠŸ: {self.bpe_model_path}")
            
        except Exception as e:
            raise RuntimeError(f"âŒ BPEæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    
    def _discover_chunks(self):
        """å‘ç°åˆ†å—æ–‡ä»¶"""
        chunk_dir = self.prepared_data_dir / f"{self.split_name}_chunks"
        
        if not chunk_dir.exists():
            raise FileNotFoundError(
                f"âŒ åˆ†å—ç›®å½•ä¸å­˜åœ¨: {chunk_dir}\n"
                f"ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python preprocess.py' è¿›è¡Œæ•°æ®é¢„å¤„ç†"
            )
        
        # å‘ç°æ‰€æœ‰åˆ†å—æ–‡ä»¶
        self.chunk_files = sorted(list(chunk_dir.glob("chunk_*.pt")))
        
        if not self.chunk_files:
            raise FileNotFoundError(
                f"âŒ æœªæ‰¾åˆ°åˆ†å—æ–‡ä»¶: {chunk_dir}\n"
                f"ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python preprocess.py' è¿›è¡Œæ•°æ®é¢„å¤„ç†"
            )
        
        # æ‰“ä¹±åˆ†å—é¡ºåºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        if self.shuffle_chunks and self.split_name == 'train':
            random.shuffle(self.chunk_files)
        
        logger.info(f"ğŸ“¦ å‘ç°{len(self.chunk_files)}ä¸ªåˆ†å—æ–‡ä»¶")
    
    def _build_sample_index(self):
        """æ„å»ºæ ·æœ¬ç´¢å¼•æ˜ å°„ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("ğŸ” æ„å»ºæ ·æœ¬ç´¢å¼•...")
        
        # ä¼˜åŒ–ï¼šä½¿ç”¨å…ƒæ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œé¿å…åŠ è½½æ‰€æœ‰åˆ†å—æ–‡ä»¶
        if self.split_name in self.metadata.get('splits', {}):
            split_info = self.metadata['splits'][self.split_name]
            self.total_samples = split_info['total_samples']
            chunks_created = split_info['chunks_created']
            
            # ä¼°ç®—æ¯ä¸ªåˆ†å—çš„æ ·æœ¬æ•°é‡ï¼ˆé™¤äº†æœ€åä¸€ä¸ªå¯èƒ½ä¸æ»¡ï¼‰
            avg_chunk_size = self.total_samples // chunks_created
            remainder = self.total_samples % chunks_created
            
            self.chunk_sample_counts = [avg_chunk_size] * chunks_created
            if remainder > 0:
                self.chunk_sample_counts[-1] += remainder
            
            logger.info(f"âœ… å¿«é€Ÿç´¢å¼•æ„å»ºå®Œæˆ: {self.total_samples:,} æ ·æœ¬ (åŸºäºå…ƒæ•°æ®)")
        else:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•ï¼ˆä»…åœ¨å…ƒæ•°æ®ä¸å¯ç”¨æ—¶ï¼‰
            logger.warning("âš ï¸ å…ƒæ•°æ®ä¸å®Œæ•´ï¼Œä½¿ç”¨æ…¢é€Ÿç´¢å¼•æ„å»º...")
            self.chunk_sample_counts = []
            self.total_samples = 0
            
            # å¿«é€Ÿæ‰«ææ¯ä¸ªåˆ†å—çš„æ ·æœ¬æ•°é‡
            for chunk_file in tqdm(self.chunk_files, desc="ğŸ“Š æ‰«æåˆ†å—"):
                try:
                    chunk_data = torch.load(chunk_file, map_location='cpu')
                    sample_count = len(chunk_data['src_data'])
                    self.chunk_sample_counts.append(sample_count)
                    self.total_samples += sample_count
                    
                except Exception as e:
                    logger.error(f"âŒ åˆ†å—æ–‡ä»¶æŸå: {chunk_file} - {str(e)}")
                    self.chunk_sample_counts.append(0)
            
            logger.info(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {self.total_samples:,} æ ·æœ¬")
        
        # æ„å»ºç´¯ç§¯ç´¢å¼•
        self.cumulative_counts = [0]
        for count in self.chunk_sample_counts:
            self.cumulative_counts.append(self.cumulative_counts[-1] + count)
    
    def _load_chunk(self, chunk_idx: int):
        """åŠ è½½æŒ‡å®šåˆ†å—åˆ°å†…å­˜"""
        if chunk_idx == self.current_chunk_idx:
            return  # å·²ç»åŠ è½½
        
        try:
            chunk_file = self.chunk_files[chunk_idx]
            chunk_data = torch.load(chunk_file, map_location='cpu')
            
            self.current_chunk_data = list(zip(
                chunk_data['src_data'], 
                chunk_data['tgt_data']
            ))
            self.current_chunk_size = len(self.current_chunk_data)
            self.current_chunk_idx = chunk_idx
            
            # æ‰“ä¹±å½“å‰åˆ†å—å†…çš„æ ·æœ¬ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
            if self.shuffle_chunks and self.split_name == 'train':
                random.shuffle(self.current_chunk_data)
            
        except Exception as e:
            logger.error(f"âŒ åˆ†å—åŠ è½½å¤±è´¥: {chunk_file} - {str(e)}")
            raise RuntimeError(f"åˆ†å—åŠ è½½å¤±è´¥: {str(e)}")
    
    def _find_chunk_and_local_idx(self, global_idx: int) -> Tuple[int, int]:
        """æ ¹æ®å…¨å±€ç´¢å¼•æ‰¾åˆ°å¯¹åº”çš„åˆ†å—å’Œå±€éƒ¨ç´¢å¼•"""
        for chunk_idx in range(len(self.cumulative_counts) - 1):
            if self.cumulative_counts[chunk_idx] <= global_idx < self.cumulative_counts[chunk_idx + 1]:
                local_idx = global_idx - self.cumulative_counts[chunk_idx]
                return chunk_idx, local_idx
        
        raise IndexError(f"å…¨å±€ç´¢å¼•è¶…å‡ºèŒƒå›´: {global_idx} >= {self.total_samples}")
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return self.total_samples
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–å•ä¸ªæ ·æœ¬"""
        if idx >= self.total_samples:
            raise IndexError(f"ç´¢å¼•è¶…å‡ºèŒƒå›´: {idx} >= {self.total_samples}")
        
        # æ‰¾åˆ°å¯¹åº”çš„åˆ†å—å’Œå±€éƒ¨ç´¢å¼•
        chunk_idx, local_idx = self._find_chunk_and_local_idx(idx)
        
        # åŠ è½½åˆ†å—ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._load_chunk(chunk_idx)
        
        # è·å–æ ·æœ¬
        src_tokens, tgt_tokens = self.current_chunk_data[local_idx]
        
        # æ·»åŠ BOS/EOS tokenï¼ˆéµå¾ªè®ºæ–‡æ ‡å‡†ï¼‰
        # æºåºåˆ—ï¼šä¸æ·»åŠ BOS/EOSï¼ˆåœ¨Encoderä¸­å¤„ç†ï¼‰
        # ç›®æ ‡åºåˆ—ï¼šæ·»åŠ BOSä½œä¸ºè¾“å…¥ï¼ŒEOSä½œä¸ºæ ‡ç­¾
        tgt_input = [config.BOS_IDX] + tgt_tokens  # Decoderè¾“å…¥
        tgt_output = tgt_tokens + [config.EOS_IDX]  # Decoderæ ‡ç­¾
        
        return {
            'src': torch.tensor(src_tokens, dtype=torch.long),
            'tgt_input': torch.tensor(tgt_input, dtype=torch.long),
            'tgt_output': torch.tensor(tgt_output, dtype=torch.long)
        }

class ChunkedDynamicBatchSampler(Sampler):
    """
    åˆ†å—åŠ¨æ€æ‰¹å¤„ç†é‡‡æ ·å™¨ - ç»ˆææ€§èƒ½ä¼˜åŒ–ç‰ˆ
    """
    def __init__(self, dataset: WMT14ChunkedDataset, max_tokens: int, shuffle: bool = True):
        self.dataset = dataset
        self.max_tokens = max_tokens
        self.shuffle = shuffle
        logger.info(f"ğŸš€ [ChunkedDynamicBatchSampler] åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“Š ç›®æ ‡tokenæ•°/æ‰¹: {max_tokens:,}")

    def __iter__(self):
        chunk_indices = list(range(len(self.dataset.chunk_files)))
        if self.shuffle:
            random.shuffle(chunk_indices)
        
        for chunk_idx in chunk_indices:
            self.dataset._load_chunk(chunk_idx)
            
            chunk_size = self.dataset.current_chunk_size
            indices_in_chunk = list(range(chunk_size))
            
            lengths_in_chunk = [max(len(src), len(tgt) + 1) for src, tgt in self.dataset.current_chunk_data]
            
            sorted_local_indices = sorted(indices_in_chunk, key=lambda i: lengths_in_chunk[i])
            
            batch_of_indices = []
            current_max_len = 0
            for local_idx in sorted_local_indices:
                global_idx = self.dataset.cumulative_counts[chunk_idx] + local_idx
                seq_len = lengths_in_chunk[local_idx]
                
                # æ›´æ–°å½“å‰æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦
                if not batch_of_indices:
                    current_max_len = seq_len
                else:
                    current_max_len = max(current_max_len, seq_len)

                if (len(batch_of_indices) + 1) * current_max_len > self.max_tokens:
                    if batch_of_indices:
                        yield batch_of_indices
                    batch_of_indices = [global_idx]
                    current_max_len = seq_len
                else:
                    batch_of_indices.append(global_idx)
            
            if batch_of_indices:
                yield batch_of_indices
    
    def __len__(self):
        # è¿™æ˜¯ä¸€ä¸ªåˆç†çš„ä¼°è®¡å€¼ï¼Œç”¨äºtqdmç­‰å·¥å…·
        # è¿™ä¸ªæ•°å­—ä¸ä¼šå½±å“å®é™…çš„è¿­ä»£æ¬¡æ•°
        if not hasattr(self, '_estimated_len'):
            try:
                avg_len = self.dataset.metadata['splits'][self.dataset.split_name]['avg_src_len']
                num_samples = self.dataset.total_samples
                sentences_per_batch = self.max_tokens / avg_len
                self._estimated_len = int(math.ceil(num_samples / sentences_per_batch))
            except:
                self._estimated_len = 65000  # å¦‚æœå…ƒæ•°æ®æœ‰é—®é¢˜ï¼Œå›é€€åˆ°ä¸€ä¸ªé»˜è®¤å€¼
        return self._estimated_len

def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """
    æ‰¹å¤„ç†æ•´ç†å‡½æ•° - é¡¹ç›®å®ªæ³•å®ç°
    
    æ ¸å¿ƒåŸåˆ™ï¼š
    1. ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥ï¼šæ­£ç¡®çš„å¡«å……å’Œæ©ç 
    2. ç»å¯¹é€‚é…ç°å®ç¡¬ä»¶ï¼šé«˜æ•ˆçš„å¼ é‡æ“ä½œ
    3. ç»å¯¹ä¿¡æ¯é€æ˜ï¼šæ¸…æ™°çš„å¼ é‡ç»´åº¦
    4. ç»å¯¹å·¥ç¨‹ä¸“ä¸šï¼šé”™è¯¯å¤„ç†
    
    Args:
        batch: æ‰¹æ¬¡æ ·æœ¬åˆ—è¡¨
        
    Returns:
        Dict: æ‰¹å¤„ç†åçš„å¼ é‡å­—å…¸
    """
    try:
        # æå–å„ä¸ªåºåˆ—
        src_seqs = [item['src'] for item in batch]
        tgt_input_seqs = [item['tgt_input'] for item in batch]
        tgt_output_seqs = [item['tgt_output'] for item in batch]
        
        # å¡«å……åºåˆ—ï¼ˆä½¿ç”¨PAD_IDXï¼‰
        src_padded = pad_sequence(src_seqs, batch_first=True, padding_value=config.PAD_IDX)
        tgt_input_padded = pad_sequence(tgt_input_seqs, batch_first=True, padding_value=config.PAD_IDX)
        tgt_output_padded = pad_sequence(tgt_output_seqs, batch_first=True, padding_value=config.PAD_IDX)
        
        # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
        # æºåºåˆ—æ©ç ï¼š1è¡¨ç¤ºæœ‰æ•ˆtokenï¼Œ0è¡¨ç¤ºPAD
        src_mask = (src_padded != config.PAD_IDX)
        
        # ç›®æ ‡åºåˆ—æ©ç ï¼š1è¡¨ç¤ºæœ‰æ•ˆtokenï¼Œ0è¡¨ç¤ºPAD
        tgt_mask = (tgt_input_padded != config.PAD_IDX)
        
        # åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        tgt_seq_len = tgt_input_padded.size(1)
        causal_mask = torch.tril(torch.ones(tgt_seq_len, tgt_seq_len, dtype=torch.bool))
        
        return {
            'src': src_padded,                    # [batch_size, src_seq_len]
            'tgt_input': tgt_input_padded,        # [batch_size, tgt_seq_len]
            'tgt_output': tgt_output_padded,      # [batch_size, tgt_seq_len]
            'src_mask': src_mask,                 # [batch_size, src_seq_len]
            'tgt_mask': tgt_mask,                 # [batch_size, tgt_seq_len]
            'causal_mask': causal_mask,           # [tgt_seq_len, tgt_seq_len]
            'batch_size': len(batch)
        }
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹å¤„ç†æ•´ç†å¤±è´¥: {str(e)}")
        raise RuntimeError(f"æ‰¹å¤„ç†æ•´ç†å¤±è´¥: {str(e)}")

def create_data_loaders() -> Tuple[DataLoader, DataLoader, DataLoader]:
    logger.info("ğŸš€ åˆ›å»ºæ•°æ®åŠ è½½å™¨ (å·²å¯ç”¨åˆ†å—åŠ¨æ€æ‰¹å¤„ç†)...")
    try:
        train_dataset = WMT14ChunkedDataset('train', shuffle_chunks=True)
        valid_dataset = WMT14ChunkedDataset('validation', shuffle_chunks=False)
        test_dataset = WMT14ChunkedDataset('test', shuffle_chunks=False)
        
        # ä½¿ç”¨æ ‡å‡† DataLoader é…åˆè‡ªå®šä¹‰ batch_sampler
        train_sampler = ChunkedDynamicBatchSampler(train_dataset, config.BATCH_SIZE_TOKENS, shuffle=True)
        
        train_loader = DataLoader(
            train_dataset,
            batch_sampler=train_sampler,
            collate_fn=collate_fn,
            num_workers=config.NUM_WORKERS,
            pin_memory=config.PIN_MEMORY
        )
        
        # éªŒè¯å’Œæµ‹è¯•é›†å¾ˆå°ï¼Œç”¨å›ºå®šæ‰¹æ¬¡å¤§å°
        valid_loader = DataLoader(
            valid_dataset,
            batch_size=config.MAX_BATCH_SIZE,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=config.NUM_WORKERS,
            pin_memory=config.PIN_MEMORY
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=config.MAX_BATCH_SIZE,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=config.NUM_WORKERS,
            pin_memory=config.PIN_MEMORY
        )
        
        logger.info(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        logger.info(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        logger.info(f"  è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬")
        logger.info(f"  éªŒè¯é›†: {len(valid_dataset):,} æ ·æœ¬")
        logger.info(f"  æµ‹è¯•é›†: {len(test_dataset):,} æ ·æœ¬")
        logger.info(f"ğŸ“¦ åˆ†å—åŠ¨æ€æ‰¹å¤„ç†é…ç½®:")
        logger.info(f"  ç›®æ ‡tokenæ•°/æ‰¹: {config.BATCH_SIZE_TOKENS:,}")
        logger.info(f"  æœ€å¤§åºåˆ—é•¿åº¦: {config.MAX_SEQ_LEN}")
        
        return train_loader, valid_loader, test_loader
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {str(e)}")
        logger.error("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        logger.error("  1. æ˜¯å¦å·²è¿è¡Œ 'python preprocess.py'")
        logger.error("  2. é¢„å¤„ç†æ•°æ®æ˜¯å¦å®Œæ•´")
        logger.error("  3. é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®")
        raise RuntimeError(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {str(e)}")

def get_vocab_info() -> Dict[str, any]:
    """
    è·å–è¯æ±‡è¡¨ä¿¡æ¯
    
    Returns:
        Dict: è¯æ±‡è¡¨ä¿¡æ¯å­—å…¸
    """
    try:
        metadata_file = Path(config.PREPARED_DATA_DIR) / "metadata.json"
        
        if not metadata_file.exists():
            raise FileNotFoundError(
                f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}\n"
                f"ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python preprocess.py' è¿›è¡Œæ•°æ®é¢„å¤„ç†"
            )
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        vocab_info = {
            'vocab_size': metadata['vocab_size'],
            'special_tokens': metadata['special_tokens'],
            'bpe_model_path': metadata['bpe_model_path'],
            'max_seq_len': metadata['max_seq_len']
        }
        
        logger.info(f"âœ… è¯æ±‡è¡¨ä¿¡æ¯è·å–æˆåŠŸ: {vocab_info['vocab_size']:,} tokens")
        return vocab_info
        
    except Exception as e:
        logger.error(f"âŒ è¯æ±‡è¡¨ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")
        raise RuntimeError(f"è¯æ±‡è¡¨ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")

def verify_data_integrity() -> bool:
    """
    éªŒè¯æ•°æ®å®Œæ•´æ€§
    
    Returns:
        bool: æ•°æ®æ˜¯å¦å®Œæ•´
    """
    logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    try:
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        prepared_data_dir = Path(config.PREPARED_DATA_DIR)
        
        required_files = [
            prepared_data_dir / "metadata.json",
            prepared_data_dir / f"{config.BPE_MODEL_PREFIX}.model"
        ]
        
        required_dirs = [
            prepared_data_dir / "train_chunks",
            prepared_data_dir / "validation_chunks",
            prepared_data_dir / "test_chunks"
        ]
        
        # æ£€æŸ¥æ–‡ä»¶
        for file_path in required_files:
            if not file_path.exists():
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file_path}")
                return False
        
        # æ£€æŸ¥ç›®å½•
        for dir_path in required_dirs:
            if not dir_path.exists():
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦ç›®å½•: {dir_path}")
                return False
            
            # æ£€æŸ¥åˆ†å—æ–‡ä»¶
            chunk_files = list(dir_path.glob("chunk_*.pt"))
            if not chunk_files:
                logger.error(f"âŒ ç›®å½•ä¸ºç©º: {dir_path}")
                return False
        
        # å°è¯•åˆ›å»ºæ•°æ®åŠ è½½å™¨
        try:
            train_loader, valid_loader, test_loader = create_data_loaders()
            
            # æµ‹è¯•åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡
            test_batch = next(iter(train_loader))
            
            logger.info(f"âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            logger.info(f"ğŸ“Š æµ‹è¯•æ‰¹æ¬¡å½¢çŠ¶:")
            logger.info(f"  src: {test_batch['src'].shape}")
            logger.info(f"  tgt_input: {test_batch['tgt_input'].shape}")
            logger.info(f"  tgt_output: {test_batch['tgt_output'].shape}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {str(e)}")
            return False
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("ğŸš€ WMT14æ•°æ®åŠ è½½å™¨æµ‹è¯• - é¡¹ç›®å®ªæ³•å®ç°")
    print("="*80)
    
    try:
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        if not verify_data_integrity():
            print("âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œ 'python preprocess.py' è¿›è¡Œæ•°æ®é¢„å¤„ç†")
            return
        
        # è·å–è¯æ±‡è¡¨ä¿¡æ¯
        vocab_info = get_vocab_info()
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {vocab_info['vocab_size']:,}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, valid_loader, test_loader = create_data_loaders()
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        print("\nğŸ§ª æµ‹è¯•æ•°æ®åŠ è½½...")
        for i, batch in enumerate(train_loader):
            if i >= 3:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
                break
            
            print(f"æ‰¹æ¬¡ {i+1}:")
            print(f"  src: {batch['src'].shape} | éé›¶å…ƒç´ : {(batch['src'] != config.PAD_IDX).sum().item()}")
            print(f"  tgt_input: {batch['tgt_input'].shape} | éé›¶å…ƒç´ : {(batch['tgt_input'] != config.PAD_IDX).sum().item()}")
            print(f"  tgt_output: {batch['tgt_output'].shape} | éé›¶å…ƒç´ : {(batch['tgt_output'] != config.PAD_IDX).sum().item()}")
        
        print("\nâœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ!")
        print("ğŸ¯ ç°åœ¨å¯ä»¥è¿è¡Œ 'python train.py' å¼€å§‹è®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()