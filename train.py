#!/usr/bin/env python3
"""
Transformerè®­ç»ƒè„šæœ¬ - BLEU 25ç›®æ ‡ä¼˜åŒ–ç‰ˆæœ¬
é¡¹ç›®å®ªæ³•ï¼šä¸¥æ ¼éµå¾ª"Attention is All You Need"è®ºæ–‡ + æ‰€æœ‰BLEUæå‡æŠ€æœ¯

æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼š
1. å¢å¼ºå­¦ä¹ ç‡è°ƒåº¦ï¼ˆWarmup + ç¼©æ”¾ï¼‰
2. æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰
3. Beam Searchè§£ç 
4. æ¢¯åº¦ç´¯ç§¯
5. æ—©åœç­–ç•¥
6. é•¿åº¦æƒ©ç½š
7. æ­£åˆ™åŒ–ï¼ˆDropoutï¼‰
8. æ£€æŸ¥ç‚¹ç®¡ç†
9. TensorBoardç›‘æ§
10. BLEUè¯„ä¼°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F

import os
import time
import math
import itertools
import json
import argparse
from typing import Dict, List, Tuple, Optional, Any
from tqdm import tqdm
import logging

# é¡¹ç›®æ¨¡å—
import config
from model import create_transformer_model
from data_utils import create_data_loaders
from beam_search import create_beam_search_decoder

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format=config.LOG_FORMAT,
    handlers=[
        logging.FileHandler('logs/train.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedTrainer:
    """
    å¢å¼ºç‰ˆTransformerè®­ç»ƒå™¨ - é›†æˆæ‰€æœ‰BLEUæå‡æŠ€æœ¯
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    - å¢å¼ºå­¦ä¹ ç‡è°ƒåº¦
    - æ ‡ç­¾å¹³æ»‘æŸå¤±
    - Beam Searchè§£ç 
    - æ¢¯åº¦ç´¯ç§¯
    - æ—©åœç­–ç•¥
    - æ£€æŸ¥ç‚¹ç®¡ç†
    - TensorBoardç›‘æ§
    - BLEUè¯„ä¼°
    """
    
    def __init__(self, resume_from: Optional[str] = None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆTransformerè®­ç»ƒå™¨")
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        os.makedirs(config.CHECKPOINTS_DIR, exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # æ•°æ®åŠ è½½å™¨
        logger.info("ğŸ“š åŠ è½½æ•°æ®...")
        self.train_loader, self.valid_loader, self.test_loader = create_data_loaders()
        
        # è·å–è¯æ±‡è¡¨ä¿¡æ¯
        from data_utils import get_vocab_info
        self.vocab_info = get_vocab_info()
        self.vocab_size = self.vocab_info['vocab_size']
        
        # åŠ è½½SentencePieceæ¨¡å‹ç”¨äºBLEUè®¡ç®—
        import sentencepiece as spm
        bpe_model_path = self.vocab_info['bpe_model_path']
        self.sp_model = spm.SentencePieceProcessor(model_file=bpe_model_path)
        
        logger.info(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        logger.info(f"ğŸ”¤ SentencePieceæ¨¡å‹å·²åŠ è½½: {bpe_model_path}")
        
        # æ¨¡å‹
        logger.info("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
        from model import create_transformer_model
        self.model = create_transformer_model(self.vocab_size).to(config.DEVICE)
        
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"ğŸ“ˆ æ¨¡å‹å‚æ•°: {total_params:,} ({total_params/1e6:.1f}M)")
        
        # æŸå¤±å‡½æ•°ï¼ˆå¸¦æ ‡ç­¾å¹³æ»‘ï¼‰
        self.criterion = nn.CrossEntropyLoss(
            label_smoothing=config.LABEL_SMOOTHING_EPS,
            ignore_index=config.PAD_IDX
        )
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=1.0,  # ä¼šè¢«å­¦ä¹ ç‡è°ƒåº¦å™¨è¦†ç›–
            betas=(0.9, 0.98),
            eps=1e-9
        )
        
        # Beam Searchè§£ç å™¨
        self.beam_decoder = create_beam_search_decoder(self.model)
        
        # TensorBoard - ä½¿ç”¨config.pyä¸­å®šä¹‰çš„æ—¥å¿—ç›®å½•
        self.writer = SummaryWriter(log_dir=config.TENSORBOARD_LOG_DIR, comment="_enhanced_transformer")
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.best_bleu = 0.0
        self.no_improvement_steps = 0
        self.start_time = time.time()
        
        # ==== æ·»åŠ æ¢å¤é€»è¾‘ ====
        if resume_from:
            if os.path.exists(resume_from):
                logger.info(f"ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {resume_from}")
                checkpoint = torch.load(resume_from, map_location=config.DEVICE)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.global_step = checkpoint['global_step']
                self.best_bleu = checkpoint.get('best_bleu', 0.0)  # ä½¿ç”¨ .get ä¿è¯å‘åå…¼å®¹
                logger.info(f"âœ… æ¢å¤æˆåŠŸ! ä»ç‰©ç†æ­¥ {self.global_step} ç»§ç»­")
            else:
                logger.error(f"âŒ æŒ‡å®šçš„æ¢å¤æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {resume_from}")
                # å¯ä»¥é€‰æ‹©é€€å‡ºæˆ–ä»å¤´å¼€å§‹
                raise FileNotFoundError(f"Checkpoint not found: {resume_from}")
        
        logger.info("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def enhanced_lr_schedule(self, step: int) -> float:
        """
        å¢å¼ºå­¦ä¹ ç‡è°ƒåº¦ - åŸºäº"Attention is All You Need"è®ºæ–‡
        
        LR = d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5)) * scale
        
        Args:
            step: å½“å‰è®­ç»ƒæ­¥æ•°
        
        Returns:
            å­¦ä¹ ç‡
        """
        d_model = config.D_MODEL
        warmup_steps = config.WARMUP_STEPS
        scale = config.LEARNING_RATE_SCALE
        
        step = max(1, step)  # é¿å…é™¤é›¶
        
        lr = (d_model ** -0.5) * min(
            step ** -0.5,
            step * (warmup_steps ** -1.5)
        ) * scale
        
        return lr
    
    def compute_loss(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—æŸå¤±
        
        Args:
            src: [seq_len, batch_size] æºåºåˆ—
            tgt: [seq_len, batch_size] ç›®æ ‡åºåˆ—
        
        Returns:
            æŸå¤±å€¼
        """
        # å‡†å¤‡è¾“å…¥å’Œè¾“å‡º
        tgt_input = tgt[:-1, :]  # ç§»é™¤æœ€åä¸€ä¸ªtokenä½œä¸ºè¾“å…¥
        tgt_output = tgt[1:, :]  # ç§»é™¤ç¬¬ä¸€ä¸ªtokenä½œä¸ºè¾“å‡º
        
        # åˆ›å»ºå› æœæ©ç 
        tgt_mask = self.model.create_causal_mask(tgt_input.size(0)).to(config.DEVICE)
        
        # å‰å‘ä¼ æ’­
        logits = self.model(src=src, tgt=tgt_input, tgt_mask=tgt_mask)
        
        # è®¡ç®—æŸå¤±
        loss = self.criterion(
            logits.reshape(-1, logits.shape[-1]),
            tgt_output.reshape(-1)
        )
        
        return loss
    
    def train_step_amp(self, batch_data: Dict[str, torch.Tensor], scaler: torch.cuda.amp.GradScaler) -> float:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤ - AMPæ··åˆç²¾åº¦ç‰ˆæœ¬
        
        Args:
            batch_data: åŒ…å«src, tgt_input, tgt_output, src_mask, tgt_mask, causal_maskçš„å­—å…¸
            scaler: AMPæ¢¯åº¦ç¼©æ”¾å™¨
        
        Returns:
            æŸå¤±å€¼
        """
        # ä»æ‰¹æ¬¡æ•°æ®ä¸­æå–å¼ é‡
        src = batch_data['src'].to(config.DEVICE)
        tgt_input = batch_data['tgt_input'].to(config.DEVICE)
        tgt_output = batch_data['tgt_output'].to(config.DEVICE)
        src_mask = batch_data.get('src_mask', None)
        tgt_mask = batch_data.get('causal_mask', None)
        
        if src_mask is not None:
            src_mask = src_mask.to(config.DEVICE)
        if tgt_mask is not None:
            tgt_mask = tgt_mask.to(config.DEVICE)
        
        # åºåˆ—é•¿åº¦é™åˆ¶
        if src.size(1) > config.MAX_SEQ_LEN:
            src = src[:, :config.MAX_SEQ_LEN]
            if src_mask is not None:
                src_mask = src_mask[:, :config.MAX_SEQ_LEN]
        
        if tgt_input.size(1) > config.MAX_SEQ_LEN:
            tgt_input = tgt_input[:, :config.MAX_SEQ_LEN]
            tgt_output = tgt_output[:, :config.MAX_SEQ_LEN]
            if tgt_mask is not None:
                tgt_mask = tgt_mask[:config.MAX_SEQ_LEN, :config.MAX_SEQ_LEN]
        
        # å­¦ä¹ ç‡è°ƒåº¦
        lr = self.enhanced_lr_schedule(self.global_step)
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        # æ¢¯åº¦ç´¯ç§¯å¼€å§‹
        if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 1:
            self.optimizer.zero_grad()
        
        # 2. ä½¿ç”¨ autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨ (ä½¿ç”¨æ–°API)
        # åœ¨è¿™ä¸ªä»£ç å—å†…ï¼Œæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„CUDAæ“ä½œéƒ½ä¼šè‡ªåŠ¨ä½¿ç”¨float16
        with torch.amp.autocast('cuda'):
            logits = self.model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)
            loss = self.criterion(
                logits.reshape(-1, logits.shape[-1]),  # [batch_size * seq_len, vocab_size]
                tgt_output.reshape(-1)  # [batch_size * seq_len]
            )
            scaled_loss = loss / config.GRADIENT_ACCUMULATION_STEPS
        
        # ä½¿ç”¨ scaler.scale() å’Œ scaler.step()
        # scaler.scale() å°†æŸå¤±ä¹˜ä»¥ç¼©æ”¾å› å­ï¼Œç„¶åè¿›è¡Œåå‘ä¼ æ’­
        scaler.scale(scaled_loss).backward()
        
        # æ¢¯åº¦æ›´æ–°
        if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 0:
            # åœ¨æ›´æ–°å‰ï¼Œåç¼©æ”¾æ¢¯åº¦ä»¥è¿›è¡Œæ¢¯åº¦è£å‰ª
            scaler.unscale_(self.optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # scaler.step() ä¼šæ£€æŸ¥æ¢¯åº¦æ˜¯å¦æº¢å‡º
            # å¦‚æœæ²¡æœ‰æº¢å‡ºï¼Œå®ƒä¼šè°ƒç”¨ optimizer.step() æ›´æ–°æƒé‡
            # å¦‚æœæº¢å‡ºäº†ï¼Œå®ƒä¼šè·³è¿‡æ­¤æ¬¡æ›´æ–°
            scaler.step(self.optimizer)
            
            # scaler.update() æ›´æ–°ç¼©æ”¾å› å­ï¼Œä¸ºä¸‹ä¸€æ¬¡è¿­ä»£åšå‡†å¤‡
            scaler.update()
        
        # è¿”å›æŸå¤±å’Œæ¢¯åº¦èŒƒæ•°ï¼ˆå¦‚æœæœ‰æ¢¯åº¦æ›´æ–°ï¼‰
        if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 0:
            return loss.item(), grad_norm.item()
        else:
            return loss.item(), None
    
    def train_step(self, batch_data: Dict[str, torch.Tensor]) -> float:
        """
        æ‰§è¡Œå•ä¸ªè®­ç»ƒæ­¥éª¤ - ä¿ç•™åŸç‰ˆæœ¬ä½œä¸ºå¤‡ç”¨
        
        Args:
            batch_data: åŒ…å«src, tgt_input, tgt_output, src_mask, tgt_mask, causal_maskçš„å­—å…¸
        
        Returns:
            æŸå¤±å€¼
        """
        # ä»æ‰¹æ¬¡æ•°æ®ä¸­æå–å¼ é‡
        src = batch_data['src'].to(config.DEVICE)
        tgt_input = batch_data['tgt_input'].to(config.DEVICE)
        tgt_output = batch_data['tgt_output'].to(config.DEVICE)
        src_mask = batch_data.get('src_mask', None)
        tgt_mask = batch_data.get('causal_mask', None)
        
        if src_mask is not None:
            src_mask = src_mask.to(config.DEVICE)
        if tgt_mask is not None:
            tgt_mask = tgt_mask.to(config.DEVICE)
        
        # åºåˆ—é•¿åº¦é™åˆ¶
        if src.size(1) > config.MAX_SEQ_LEN:
            src = src[:, :config.MAX_SEQ_LEN]
            if src_mask is not None:
                src_mask = src_mask[:, :config.MAX_SEQ_LEN]
        
        if tgt_input.size(1) > config.MAX_SEQ_LEN:
            tgt_input = tgt_input[:, :config.MAX_SEQ_LEN]
            tgt_output = tgt_output[:, :config.MAX_SEQ_LEN]
            if tgt_mask is not None:
                tgt_mask = tgt_mask[:config.MAX_SEQ_LEN, :config.MAX_SEQ_LEN]
        
        # å­¦ä¹ ç‡è°ƒåº¦
        lr = self.enhanced_lr_schedule(self.global_step)
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
        
        # æ¢¯åº¦ç´¯ç§¯å¼€å§‹
        if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 1:
            self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        logits = self.model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)
        
        # è®¡ç®—æŸå¤± - æ­£ç¡®reshape logitså’Œtarget
        loss = self.criterion(
            logits.reshape(-1, logits.shape[-1]),  # [batch_size * seq_len, vocab_size]
            tgt_output.reshape(-1)  # [batch_size * seq_len]
        )
        
        # æ¢¯åº¦ç´¯ç§¯ç¼©æ”¾
        scaled_loss = loss / config.GRADIENT_ACCUMULATION_STEPS
        scaled_loss.backward()
        
        # æ¢¯åº¦æ›´æ–°
        if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 0:
            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
        
        # è¿”å›æŸå¤±å’Œæ¢¯åº¦èŒƒæ•°ï¼ˆå¦‚æœæœ‰æ¢¯åº¦æ›´æ–°ï¼‰
        if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 0:
            return loss.item(), grad_norm.item()
        else:
            return loss.item(), None
    
    def evaluate_bleu(self, dataloader: DataLoader, max_samples: int = 100) -> float:
        """
        BLEUè¯„ä¼° - ä½¿ç”¨Beam Searchï¼ˆç¬¦åˆè®ºæ–‡æ ‡å‡†ï¼‰
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
        
        Returns:
            BLEUåˆ†æ•°
        """
        try:
            from sacrebleu import corpus_bleu
            from beam_search import BeamSearchDecoder
        except ImportError:
            logger.warning("sacrebleuæˆ–beam_searchæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–BLEUè¯„ä¼°")
            return self.simple_bleu_evaluation(dataloader, max_samples)
        
        self.model.eval()
        references = []
        hypotheses = []
        
        # åˆ›å»ºBeam Searchè§£ç å™¨ï¼ˆè®ºæ–‡æ ‡å‡†ï¼šbeam_size=4ï¼‰
        beam_decoder = BeamSearchDecoder(
            model=self.model,
            beam_size=4,  # è®ºæ–‡æ ‡å‡†
            max_length=config.MAX_DECODE_LENGTH,
            length_penalty=0.6,  # è®ºæ–‡æ ‡å‡†
            early_stopping=True
        )
        
        count = 0
        sample_count = 0
        with torch.no_grad():
            # éå†æ‰¹æ¬¡
            for batch_data in tqdm(dataloader, desc="ğŸ“Š BLEUè¯„ä¼° (Beam Search)", leave=False):
                if count >= max_samples:
                    break
                
                # ä»æ‰¹æ¬¡ä¸­æå–å¼ é‡
                src_batch = batch_data['src'].to(config.DEVICE)
                tgt_batch = batch_data['tgt_output'].to(config.DEVICE)
                src_mask_batch = batch_data.get('src_mask', None)
                if src_mask_batch is not None:
                    src_mask_batch = src_mask_batch.to(config.DEVICE)

                # ä¼˜åŒ–ï¼šåªå¤„ç†æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œé¿å…é‡å¤ç”Ÿæˆ
                batch_size = min(src_batch.size(0), max_samples - count)
                for i in range(batch_size):
                    if count >= max_samples:
                        break

                    # è·å–å•ä¸ªæ ·æœ¬ï¼Œå¹¶ä¿æŒæ‰¹æ¬¡ç»´åº¦ [1, seq_len]
                    src_sentence = src_batch[i:i+1]
                    src_mask = src_mask_batch[i:i+1] if src_mask_batch is not None else None
                    
                    # ä½¿ç”¨Beam Searchè§£ç ï¼ˆè®ºæ–‡æ ‡å‡†ï¼‰
                    try:
                        beam_result = beam_decoder.search(src_sentence, src_mask, verbose=False)
                        pred_tokens = beam_result['sequences'][0] if beam_result['sequences'] else []
                    except Exception as e:
                        logger.warning(f"Beam Searchè§£ç å¤±è´¥: {e}ï¼Œå›é€€åˆ°è´ªå¿ƒè§£ç ")
                        pred_tokens = self.greedy_decode(src_sentence, src_mask)
                    
                    # è½¬æ¢ä¸ºæ–‡æœ¬
                    src_tokens = src_batch[i].cpu().numpy()
                    tgt_tokens = tgt_batch[i].cpu().numpy()
                    
                    src_text = self._tokens_to_text(src_tokens, self.vocab_size)
                    ref_text = self._tokens_to_text(tgt_tokens, self.vocab_size)
                    hyp_text = self._tokens_to_text(pred_tokens, self.vocab_size)
                    
                    # æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬ç”¨äºè°ƒè¯•
                    if sample_count < 2:
                        logger.info(f"\nğŸ“ æ ·æœ¬ {sample_count + 1} (Beam Search):")
                        logger.info(f"   æºæ–‡æœ¬: {src_text}")
                        logger.info(f"   å‚è€ƒè¯‘æ–‡: {ref_text}")
                        logger.info(f"   æ¨¡å‹è¯‘æ–‡: {hyp_text}")
                        sample_count += 1
                    
                    if ref_text.strip() and hyp_text.strip():  # ç¡®ä¿ä¸¤è€…éƒ½ä¸ä¸ºç©º
                        references.append(ref_text)
                        hypotheses.append(hyp_text)
                    
                    count += 1
        
        if not references or not hypotheses or len(hypotheses) != len(references):
            logger.warning("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ç¿»è¯‘å¯¹æˆ–æ•°é‡ä¸åŒ¹é…ï¼Œè¿”å›0 BLEU")
            return 0.0
        
        logger.info(f"ğŸ“Š æ”¶é›†äº† {len(references)} ä¸ªæœ‰æ•ˆç¿»è¯‘å¯¹ (Beam Search)")
        
        # è®¡ç®—BLEU
        try:
            # sacrebleu æœŸæœ›çš„æ ¼å¼æ˜¯ï¼šhypothesesæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œreferencesæ˜¯ä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨
            bleu = corpus_bleu(hypotheses, [references])
            logger.info(f"ğŸ“Š BLEUè¯¦æƒ…: {bleu}")
            return bleu.score
        except Exception as e:
            logger.warning(f"âŒ BLEUè®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def simple_bleu_evaluation(self, dataloader: DataLoader, max_samples: int = 50) -> float:
        """
        ç®€åŒ–BLEUè¯„ä¼°ï¼ˆå½“sacrebleuä¸å¯ç”¨æ—¶ï¼‰
        """
        self.model.eval()
        total_score = 0.0
        count = 0
        
        with torch.no_grad():
            for batch_data in dataloader:
                if count >= max_samples:
                    break
                
                src = batch_data['src'].to(config.DEVICE)
                tgt_output = batch_data['tgt_output'].to(config.DEVICE)
                
                # ç¼–ç 
                src_mask = batch_data.get('src_mask', None)
                if src_mask is not None:
                    src_mask = src_mask.to(config.DEVICE)
                
                # è´ªå¿ƒè§£ç 
                pred_tokens = self.greedy_decode(src[:1], src_mask[:1] if src_mask is not None else None)
                tgt_tokens = tgt_output[0].cpu().numpy()  # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                
                # ç®€å•çš„tokenåŒ¹é…åˆ†æ•°
                matches = sum(1 for p, t in zip(pred_tokens, tgt_tokens) if p == t)
                total_score += matches / max(len(pred_tokens), len(tgt_tokens), 1)
                count += 1
        
        return (total_score / max(count, 1)) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    
    def greedy_decode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None, max_length: int = 100) -> List[int]:
        """
        è´ªå¿ƒè§£ç ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            src: æºåºåˆ— [1, src_seq_len]
            src_mask: æºåºåˆ—æ©ç 
            max_length: æœ€å¤§è§£ç é•¿åº¦
        
        Returns:
            è§£ç çš„tokenåºåˆ—
        """
        self.model.eval()
        
        # ç¼–ç 
        memory = self.model.encode(src, src_mask)
        
        # è§£ç 
        ys = torch.ones(1, 1).fill_(config.BOS_IDX).type(torch.long).to(config.DEVICE)
        
        for _ in range(max_length):
            tgt_mask = self.model.create_causal_mask(ys.size(1)).to(config.DEVICE)
            out = self.model.decode(ys, memory, tgt_mask)
            prob = F.softmax(out[:, -1], dim=-1)
            next_word = torch.argmax(prob, dim=-1).item()
            
            if next_word == config.EOS_IDX:
                break
            
            ys = torch.cat([ys, torch.ones(1, 1).type(torch.long).fill_(next_word).to(config.DEVICE)], dim=1)
        
        return ys[0, 1:].cpu().tolist()  # ç§»é™¤BOS
    
    def _tokens_to_text(self, tokens: List[int], vocab_size: int) -> str:
        """
        å°†tokenè½¬æ¢ä¸ºæ–‡æœ¬ç”¨äºBLEUè®¡ç®—
        
        Args:
            tokens: tokenåºåˆ—
            vocab_size: è¯æ±‡è¡¨å¤§å°
            
        Returns:
            str: è§£ç åçš„æ–‡æœ¬
        """
        try:
            # è¿‡æ»¤ç‰¹æ®Štoken
            filtered_tokens = []
            for token in tokens:
                if isinstance(token, torch.Tensor):
                    token = token.item()
                token_int = int(token)
                if token_int not in [config.PAD_IDX, config.UNK_IDX, config.BOS_IDX, config.EOS_IDX]:
                    filtered_tokens.append(token_int)
            
            if not filtered_tokens:
                return ""
            
            # ä½¿ç”¨SentencePieceè§£ç 
            if hasattr(self, 'sp_model'):
                text = self.sp_model.decode_ids(filtered_tokens)
                # æ›¿æ¢â–ä¸ºç©ºæ ¼ï¼Œè¿™æ˜¯SentencePieceçš„æ ‡å‡†åšæ³•
                text = text.replace('â–', ' ').strip()
                return text
            else:
                logger.warning("SentencePieceæ¨¡å‹æœªåŠ è½½")
                return ""
            
        except Exception as e:
            logger.warning(f"æ–‡æœ¬è§£ç å¤±è´¥: {str(e)}")
            return ""
    
    def validate(self, skip_bleu: bool = False) -> Tuple[float, float]:
        """
        éªŒè¯æ¨¡å‹
        
        Args:
            skip_bleu: æ˜¯å¦è·³è¿‡BLEUè¯„ä¼°
        
        Returns:
            (éªŒè¯æŸå¤±, BLEUåˆ†æ•°)
        """
        logger.info(f"ğŸ” éªŒè¯æ¨¡å‹ (Step {self.global_step})")
        
        self.model.eval()
        total_loss = 0.0
        count = 0
        
        with torch.no_grad():
            for batch_data in tqdm(self.valid_loader, desc="ğŸ” éªŒè¯ä¸­", leave=False):
                if count >= 100:  # é™åˆ¶éªŒè¯æ—¶é—´
                    break
                
                src = batch_data['src'].to(config.DEVICE)
                tgt_input = batch_data['tgt_input'].to(config.DEVICE)
                tgt_output = batch_data['tgt_output'].to(config.DEVICE)
                src_mask = batch_data.get('src_mask', None)
                tgt_mask = batch_data.get('causal_mask', None)
                
                if src_mask is not None:
                    src_mask = src_mask.to(config.DEVICE)
                if tgt_mask is not None:
                    tgt_mask = tgt_mask.to(config.DEVICE)
                
                # å‰å‘ä¼ æ’­
                logits = self.model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)
                
                # è®¡ç®—æŸå¤± - æ­£ç¡®reshape logitså’Œtarget
                loss = self.criterion(
                    logits.reshape(-1, logits.shape[-1]),  # [batch_size * seq_len, vocab_size]
                    tgt_output.reshape(-1)  # [batch_size * seq_len]
                )
                total_loss += loss.item()
                count += 1
        
        avg_loss = total_loss / max(count, 1)
        
        # BLEUè¯„ä¼°ï¼ˆå¯é€‰ï¼‰
        if skip_bleu:
            bleu_score = 0.0
            logger.info(f"ğŸ“‰ éªŒè¯æŸå¤±: {avg_loss:.4f}")
            logger.info(f"â­ï¸ è·³è¿‡BLEUè¯„ä¼°")
        else:
            bleu_score = self.evaluate_bleu(self.valid_loader, max_samples=50)
            logger.info(f"ğŸ“‰ éªŒè¯æŸå¤±: {avg_loss:.4f}")
            logger.info(f"ğŸ“Š BLEUåˆ†æ•°: {bleu_score:.2f}")
        
        # TensorBoardè®°å½•ï¼ˆä½¿ç”¨é€»è¾‘æ­¥ï¼‰
        logical_step = self.global_step // config.GRADIENT_ACCUMULATION_STEPS
        self.writer.add_scalar('Validation/Loss', avg_loss, logical_step)
        if not skip_bleu:
            self.writer.add_scalar('Validation/BLEU', bleu_score, logical_step)
        
        return avg_loss, bleu_score
    
    def save_checkpoint(self, bleu_score: float, val_loss: float, is_best: bool = False):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        """
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'global_step': self.global_step,
            'best_bleu': self.best_bleu,
            'bleu_score': bleu_score,
            'val_loss': val_loss,
            'config': {
                'src_vocab_size': self.vocab_size,
                'tgt_vocab_size': self.vocab_size,
                'd_model': config.D_MODEL,
                'nhead': config.NHEAD,
                'num_encoder_layers': config.NUM_ENCODER_LAYERS,
                'num_decoder_layers': config.NUM_DECODER_LAYERS,
                'dim_feedforward': config.DIM_FEEDFORWARD,
                'dropout': config.DROPOUT
            }
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(config.CHECKPOINTS_DIR, 'latest_model.pt')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(config.CHECKPOINTS_DIR, f'best_model_bleu{bleu_score:.1f}.pt')
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # å®šæœŸä¿å­˜ï¼ˆåŸºäºé€»è¾‘æ­¥ï¼‰
        logical_step = self.global_step // config.GRADIENT_ACCUMULATION_STEPS
        if logical_step % config.SAVE_EVERY_LOGICAL_STEPS == 0:
            step_path = os.path.join(config.CHECKPOINTS_DIR, f'model_logical_step_{logical_step}.pt')
            torch.save(checkpoint, step_path)
    
    def train(self):
        """
        ä¸»è®­ç»ƒå¾ªç¯ - [å·²é›†æˆAMPæ··åˆç²¾åº¦è®­ç»ƒ]
        """
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ (å·²å¯ç”¨æ··åˆç²¾åº¦AMP)")
        logger.info(f"ğŸ¯ ç›®æ ‡: BLEU â‰¥ 25.0")
        logger.info(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {config.TRAIN_STEPS:,}")
        logger.info(f"ğŸ“Š éªŒè¯é¢‘ç‡: æ¯ {config.VALIDATE_EVERY_LOGICAL_STEPS} é€»è¾‘æ­¥")
        logger.info(f"ğŸ“Š æ—©åœè€å¿ƒ: {config.EARLY_STOPPING_PATIENCE} é€»è¾‘æ­¥")
        
        # 1. åˆå§‹åŒ– GradScaler (ä½¿ç”¨æ–°APIé¿å…è­¦å‘Š)
        # GradScaler ç”¨äºè‡ªåŠ¨è¿›è¡Œæ¢¯åº¦ç¼©æ”¾ï¼Œé˜²æ­¢float16æ¢¯åº¦ä¸‹æº¢
        scaler = torch.amp.GradScaler('cuda')
        
        self.model.train()
        train_iter = itertools.cycle(self.train_loader)
        logical_step = 0  # é€»è¾‘æ›´æ–°æ­¥æ•°
        
        # ä½¿ç”¨tqdmä½œä¸ºä¸»å¾ªç¯çš„è¿›åº¦æ¡ï¼Œç›‘æ§ç‰©ç†æ­¥
        progress_bar = tqdm(range(1, config.TRAIN_STEPS + 1), desc="ğŸš€ è®­ç»ƒä¸­", unit="step")
        
        for self.global_step in progress_bar:
            batch_data = next(train_iter)
            train_result = self.train_step_amp(batch_data, scaler)  # ä½¿ç”¨AMPç‰ˆæœ¬çš„è®­ç»ƒæ­¥éª¤
            
            # å¤„ç†è¿”å›å€¼
            if isinstance(train_result, tuple):
                loss, grad_norm = train_result
            else:
                loss = train_result
                grad_norm = None
            
            # ================= æ ¸å¿ƒä¿®æ­£ï¼šæ‰€æœ‰é€»è¾‘éƒ½åŸºäºå‚æ•°æ›´æ–°ç‚¹ =================
            if self.global_step % config.GRADIENT_ACCUMULATION_STEPS == 0:
                logical_step += 1
                
                # --- æ›´æ–°TensorBoard ---
                self.writer.add_scalar('Training/Loss', loss, logical_step)
                lr = self.optimizer.param_groups[0]['lr']
                self.writer.add_scalar('Training/LearningRate', lr, logical_step)
                if grad_norm is not None:
                    self.writer.add_scalar('Training/GradNorm', grad_norm, logical_step)
                
                # --- æ›´æ–°è¿›åº¦æ¡æè¿° ---
                progress_bar.set_postfix({
                    'Logical Step': logical_step,
                    'Loss': f'{loss:.4f}',
                    'LR': f'{lr:.2e}',
                    'Best BLEU': f'{self.best_bleu:.3f}'
                })
                
                # --- å®šæœŸæ—¥å¿— (åŸºäºé€»è¾‘æ­¥) ---
                if logical_step > 0 and logical_step % config.LOG_EVERY_LOGICAL_STEPS == 0:
                    elapsed = time.time() - self.start_time
                    progress = self.global_step / config.TRAIN_STEPS * 100
                    steps_per_sec = self.global_step / elapsed if elapsed > 0 else 0
                    eta_seconds = (config.TRAIN_STEPS - self.global_step) / steps_per_sec if steps_per_sec > 0 else 0
                    eta_hours = eta_seconds / 3600
                    
                    logger.info(
                        f"ğŸ“Š Step {self.global_step:6d}/{config.TRAIN_STEPS} ({progress:5.1f}%) | "
                        f"Logical: {logical_step} | "
                        f"Loss: {loss:.4f} | LR: {lr:.2e} | "
                        f"Speed: {steps_per_sec:.1f} steps/s | "
                        f"ETA: {eta_hours:.1f}h | "
                        f"Best BLEU: {self.best_bleu:.3f}"
                    )
                
                # --- å®šæœŸéªŒè¯ (åŸºäºé€»è¾‘æ­¥) ---
                if logical_step > 0 and logical_step % config.VALIDATE_EVERY_LOGICAL_STEPS == 0:
                    logger.info(f"ğŸ” å¼€å§‹ç¬¬ {logical_step} é€»è¾‘æ­¥éªŒè¯è¯„ä¼°...")
                    
                    # åœ¨è®­ç»ƒåˆæœŸè·³è¿‡æ˜‚è´µçš„BLEUè¯„ä¼°
                    if self.global_step < config.SKIP_BLEU_BEFORE_STEPS:
                        logger.info(f"â­ï¸ è·³è¿‡BLEUè¯„ä¼° (æ­¥æ•° {self.global_step} < {config.SKIP_BLEU_BEFORE_STEPS})")
                        val_loss, _ = self.validate(skip_bleu=True)
                        bleu_score = 0.0
                    else:
                        val_loss, bleu_score = self.validate()
                    
                    # æ£€æŸ¥æ”¹è¿›ï¼ˆä½¿ç”¨é…ç½®çš„æœ€å°æå‡é˜ˆå€¼ï¼‰
                    improvement = bleu_score - self.best_bleu
                    is_best = improvement > config.MIN_DELTA_BLEU
                    
                    logger.info(f"ğŸ“‹ éªŒè¯ç»“æœ: Loss={val_loss:.4f}, BLEU={bleu_score:.3f}")
                    
                    if is_best:
                        self.best_bleu = bleu_score
                        self.no_improvement_steps = 0
                        logger.info(f"ğŸ‰ æ–°çºªå½•! BLEU: {self.best_bleu:.3f} (+{improvement:.3f})")
                        
                        if self.best_bleu >= 25.0:
                            logger.info(f"ğŸ† è¾¾æˆç›®æ ‡! BLEU {self.best_bleu:.3f} â‰¥ 25.0")
                            self.save_checkpoint(bleu_score, val_loss, is_best=True)
                            break
                    else:
                        self.no_improvement_steps += config.VALIDATE_EVERY_LOGICAL_STEPS
                        if improvement > 0:
                            logger.info(f"ğŸ“ˆ å°å¹…æå‡: BLEU {bleu_score:.3f} (+{improvement:.3f}, éœ€>{config.MIN_DELTA_BLEU:.3f})")
                        else:
                            logger.info(f"ğŸ“‰ æ— æå‡: BLEU {bleu_score:.3f} ({improvement:+.3f})")
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    self.save_checkpoint(bleu_score, val_loss, is_best=is_best)
                    
                    # æ—©åœæ£€æŸ¥ï¼ˆä½¿ç”¨é…ç½®çš„è€å¿ƒå‚æ•°ï¼ŒåŸºäºé€»è¾‘æ­¥ï¼‰
                    patience_logical_steps = config.EARLY_STOPPING_PATIENCE * config.VALIDATE_EVERY_LOGICAL_STEPS
                    if self.no_improvement_steps >= patience_logical_steps:
                        logger.info(f"â¹ï¸ æ—©åœ: è¿ç»­ {self.no_improvement_steps} é€»è¾‘æ­¥æ— æ”¹å–„ (è€å¿ƒ: {patience_logical_steps} é€»è¾‘æ­¥)")
                        break
                    
                    self.model.train()
                
                # --- å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ (åŸºäºé€»è¾‘æ­¥) ---
                if logical_step > 0 and logical_step % config.SAVE_EVERY_LOGICAL_STEPS == 0:
                    self.save_checkpoint(0.0, 0.0, is_best=False)  # å®šæœŸä¿å­˜
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if self.best_bleu >= 25.0:
                break
            
            # GPUå†…å­˜æ¸…ç†
            if self.global_step % 1000 == 0:
                torch.cuda.empty_cache()
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - self.start_time
        logger.info("ğŸ è®­ç»ƒå®Œæˆ!")
        logger.info(f"ğŸ¯ æœ€ä½³BLEU: {self.best_bleu:.2f}")
        logger.info(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.1f}å°æ—¶")
        logger.info(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {self.global_step:,}")
        logger.info(f"ğŸ“Š æ€»é€»è¾‘æ­¥æ•°: {logical_step:,}")
        
        if self.best_bleu >= 25.0:
            logger.info("ğŸ‰ SUCCESS! è¾¾æˆBLEU â‰¥ 25.0 ç›®æ ‡!")
        else:
            logger.info("âš ï¸ æœªè¾¾ç›®æ ‡ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–ä½¿ç”¨æ¨¡å‹é›†æˆ")
        
        self.writer.close()
        return self.best_bleu
    
    def test(self) -> float:
        """
        æµ‹è¯•æœ€ä½³æ¨¡å‹
        
        Returns:
            æµ‹è¯•é›†BLEUåˆ†æ•°
        """
        logger.info("ğŸ§ª æµ‹è¯•æœ€ä½³æ¨¡å‹")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = os.path.join(config.CHECKPOINTS_DIR, 'latest_model.pt')
        if os.path.exists(best_model_path):
            checkpoint = torch.load(best_model_path, map_location=config.DEVICE)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {best_model_path}")
        else:
            logger.warning("æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹")
        
        # æµ‹è¯•è¯„ä¼°
        test_bleu = self.evaluate_bleu(self.test_loader, max_samples=200)
        logger.info(f"ğŸ“Š æµ‹è¯•é›†BLEU: {test_bleu:.2f}")
        
        return test_bleu

def main():
    """
    ä¸»å‡½æ•° - [å·²é›†æˆå‘½ä»¤è¡Œå‚æ•°]
    """
    parser = argparse.ArgumentParser(description='Transformerè®­ç»ƒä¸æµ‹è¯•è„šæœ¬')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test', 'resume'],
                       help='è¿è¡Œæ¨¡å¼: "train" (è®­ç»ƒ), "test" (æµ‹è¯•), æˆ– "resume" (æ¢å¤è®­ç»ƒ)')
    parser.add_argument('--checkpoint', type=str, default=None,
                       help='æµ‹è¯•æˆ–æ¢å¤æ—¶è¦åŠ è½½çš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ Transformerè®­ç»ƒ - BLEU 25ç›®æ ‡ä¼˜åŒ–ç‰ˆæœ¬")
    print("="*80)
    print("ğŸ“‹ é¡¹ç›®å®ªæ³•åŸåˆ™:")
    print("  âœ… ç»å¯¹å¿ äºåŸæ–‡ç²¾ç¥: ä¸¥æ ¼éµå¾ª'Attention is All You Need'è®ºæ–‡")
    print("  âœ… å·¥ç¨‹å®è·µè‡³ä¸Š: é›†æˆæ‰€æœ‰BLEUæå‡æŠ€æœ¯")
    print("  âœ… æ€§èƒ½å¯¼å‘: ç›®æ ‡BLEU â‰¥ 25.0")
    print("  âœ… ä¿¡æ¯é€æ˜: è¯¦ç»†æ—¥å¿— + TensorBoardç›‘æ§")
    print("  âœ… ç¨³å®šå¯é : æ£€æŸ¥ç‚¹ä¿å­˜ + æ—©åœç­–ç•¥")
    print("="*80)
    print(f"ğŸ¯ æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯:")
    print(f"  ğŸ“ˆ å¢å¼ºå­¦ä¹ ç‡è°ƒåº¦: Warmup={config.WARMUP_STEPS}, Scale={config.LEARNING_RATE_SCALE}")
    print(f"  ğŸ­ æ ‡ç­¾å¹³æ»‘: Îµ={config.LABEL_SMOOTHING_EPS}")
    print(f"  ğŸ” Beam Search: size={config.BEAM_SIZE}, Î±={config.LENGTH_PENALTY}")
    print(f"  ğŸ“Š æ¢¯åº¦ç´¯ç§¯: {config.GRADIENT_ACCUMULATION_STEPS} æ­¥")
    print(f"  â¹ï¸ æ—©åœç­–ç•¥: {config.EARLY_STOPPING_PATIENCE} é€»è¾‘æ­¥è€å¿ƒ")
    print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹: æ¯ {config.SAVE_EVERY_LOGICAL_STEPS} é€»è¾‘æ­¥ä¿å­˜")
    print(f"  ğŸš€ æ··åˆç²¾åº¦è®­ç»ƒ: AMP (RTX 4060ä¼˜åŒ–)")
    print(f"  ğŸ“Š éªŒè¯é¢‘ç‡: æ¯ {config.VALIDATE_EVERY_LOGICAL_STEPS} é€»è¾‘æ­¥")
    print(f"  ğŸ“Š æ—¥å¿—é¢‘ç‡: æ¯ {config.LOG_EVERY_LOGICAL_STEPS} é€»è¾‘æ­¥")
    print("="*80)
    
    try:
        # å°†argsä¼ ç»™Trainer
        trainer = EnhancedTrainer(resume_from=args.checkpoint if args.mode == 'resume' else None)
        
        if args.mode == 'train' or args.mode == 'resume':
            # å¼€å§‹è®­ç»ƒ
            best_bleu = trainer.train()
            # è®­ç»ƒç»“æŸåè‡ªåŠ¨æµ‹è¯•æœ€ä½³æ¨¡å‹
            logger.info("\nğŸ è®­ç»ƒå®Œæˆï¼Œå¼€å§‹ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
            test_bleu = trainer.test()
            
            print(f"\nğŸ† æœ€ç»ˆç»“æœ:")
            print(f"  ğŸ¯ æœ€ä½³éªŒè¯BLEU: {best_bleu:.2f}")
            print(f"  ğŸ§ª æµ‹è¯•é›†BLEU: {test_bleu:.2f}")
            
            if best_bleu >= 25.0:
                print(f"ğŸ‰ SUCCESS! è¾¾æˆBLEU â‰¥ 25.0 ç›®æ ‡!")
            else:
                print(f"âš ï¸ æœªè¾¾ç›®æ ‡ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–ä½¿ç”¨æ¨¡å‹é›†æˆ")
        
        elif args.mode == 'test':
            if not args.checkpoint:
                logger.error("âŒ æµ‹è¯•æ¨¡å¼éœ€è¦æŒ‡å®šæ£€æŸ¥ç‚¹è·¯å¾„ï¼Œè¯·ä½¿ç”¨ --checkpoint /path/to/model.pt")
                return
            
            logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼ï¼ŒåŠ è½½æ£€æŸ¥ç‚¹: {args.checkpoint}")
            checkpoint = torch.load(args.checkpoint, map_location=config.DEVICE)
            # å…¼å®¹ average_checkpoints.py çš„è¾“å‡ºæ ¼å¼
            state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
            trainer.model.load_state_dict(state_dict)
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_bleu = trainer.evaluate_bleu(trainer.test_loader, max_samples=config.MAX_EVAL_SAMPLES)
            logger.info(f"ğŸ† æµ‹è¯•é›†æœ€ç»ˆBLEUåˆ†æ•°: {test_bleu:.3f}")
            
            print(f"\nğŸ† æµ‹è¯•ç»“æœ:")
            print(f"  ğŸ§ª æµ‹è¯•é›†BLEU: {test_bleu:.3f}")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        raise

if __name__ == "__main__":
    main()