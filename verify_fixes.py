#!/usr/bin/env python3
"""
éªŒè¯æ‰€æœ‰ä¿®å¤æ˜¯å¦æ­£ç¡®å·¥ä½œ
æ£€æŸ¥æŸå¤±è®¡ç®—ã€å­¦ä¹ ç‡è°ƒåº¦å’Œè®­ç»ƒé€»è¾‘
"""

import torch
import torch.nn as nn
import config
import math
from train import LabelSmoothingLoss, WarmupLRScheduler
from data_utils import create_data_loaders
from model import create_transformer_model

def test_label_smoothing_loss():
    """æµ‹è¯•æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°"""
    print("ğŸ” æµ‹è¯•æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°...")
    
    vocab_size = 37000
    criterion = LabelSmoothingLoss(vocab_size, config.PAD_IDX, 0.1)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 2, 5
    pred = torch.randn(batch_size, seq_len, vocab_size) * 0.1
    target = torch.tensor([
        [100, 200, 300, config.PAD_IDX, config.PAD_IDX],
        [400, 500, 600, 700, 800]
    ])
    
    loss = criterion(pred, target)
    print(f"  æŸå¤±å€¼: {loss.item():.4f}")
    print(f"  ç†è®ºèŒƒå›´: 6.9-10.5 (ln(1000) - ln(37000))")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´
    if 5.0 <= loss.item() <= 15.0:
        print("  âœ… æŸå¤±å€¼åœ¨åˆç†èŒƒå›´å†…")
    else:
        print("  âŒ æŸå¤±å€¼å¼‚å¸¸")
    
    return loss.item()

def test_warmup_scheduler():
    """æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    print("\nğŸ” æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å™¨...")
    
    # åˆ›å»ºè™šæ‹Ÿä¼˜åŒ–å™¨
    model = nn.Linear(10, 10)
    optimizer = torch.optim.Adam(model.parameters(), lr=1.0)
    
    scheduler = WarmupLRScheduler(optimizer, config.D_MODEL, config.WARMUP_STEPS)
    
    # æµ‹è¯•é¢„çƒ­é˜¶æ®µ
    print("  é¢„çƒ­é˜¶æ®µ:")
    for step in [1, 1000, 2000, 4000]:
        scheduler.step_num = step - 1
        lr = scheduler.step()
        print(f"    Step {step}: LR = {lr:.8f}")
    
    # æµ‹è¯•è¡°å‡é˜¶æ®µ
    print("  è¡°å‡é˜¶æ®µ:")
    for step in [5000, 8000, 16000]:
        scheduler.step_num = step - 1
        lr = scheduler.step()
        print(f"    Step {step}: LR = {lr:.8f}")
    
    print("  âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥ä½œæ­£å¸¸")

def test_training_step():
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤"""
    print("\nğŸ” æµ‹è¯•è®­ç»ƒæ­¥éª¤...")
    
    # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®
    model = create_transformer_model(config.BPE_VOCAB_SIZE)
    model.eval()
    
    criterion = LabelSmoothingLoss(config.BPE_VOCAB_SIZE, config.PAD_IDX, config.LABEL_SMOOTHING_EPS)
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    train_loader, _, _ = create_data_loaders()
    batch = next(iter(train_loader))
    
    src = batch['src']
    tgt_input = batch['tgt_input']
    tgt_output = batch['tgt_output']
    
    print(f"  æ‰¹æ¬¡å¤§å°: {src.size(0)}")
    print(f"  åºåˆ—é•¿åº¦: src={src.size(1)}, tgt={tgt_input.size(1)}")
    
    with torch.no_grad():
        # åˆ›å»ºæ©ç 
        src_mask = model.create_padding_mask(src, config.PAD_IDX)
        tgt_mask = model.create_causal_mask(tgt_input.size(1))
        
        # å‰å‘ä¼ æ’­
        output = model(src, tgt_input, src_mask, tgt_mask)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(output, tgt_output)
        
        print(f"  æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  æŸå¤±å€¼: {loss.item():.4f}")
        
        # æ¨¡æ‹Ÿæ¢¯åº¦ç´¯ç§¯
        original_loss = loss.item()
        scaled_loss = loss / config.GRADIENT_ACCUMULATION_STEPS
        
        print(f"  åŸå§‹æŸå¤±: {original_loss:.4f}")
        print(f"  ç¼©æ”¾æŸå¤±: {scaled_loss.item():.4f}")
        print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.GRADIENT_ACCUMULATION_STEPS}")
        
        if 200 <= original_loss <= 300:
            print("  âœ… è®­ç»ƒæ­¥éª¤å·¥ä½œæ­£å¸¸")
        else:
            print(f"  âš ï¸ æŸå¤±å€¼å¯èƒ½å¼‚å¸¸: {original_loss:.4f}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª éªŒè¯æ‰€æœ‰ä¿®å¤")
    print("=" * 60)
    
    try:
        # æµ‹è¯•æŸå¤±å‡½æ•°
        loss_value = test_label_smoothing_loss()
        
        # æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å™¨
        test_warmup_scheduler()
        
        # æµ‹è¯•è®­ç»ƒæ­¥éª¤
        test_training_step()
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print("âœ… æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°: æ­£å¸¸")
        print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨: æ­£å¸¸")
        print("âœ… è®­ç»ƒæ­¥éª¤: æ­£å¸¸")
        print("âœ… æ¢¯åº¦ç´¯ç§¯é€»è¾‘: æ­£å¸¸")
        print("âœ… æ—¥å¿—è¾“å‡ºé¢‘ç‡: å·²ä¿®æ”¹ä¸ºæ¯100æ­¥")
        print("\nğŸ‰ æ‰€æœ‰ä¿®å¤éªŒè¯é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()